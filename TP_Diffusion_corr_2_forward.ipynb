{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qqmmsIDUOD0"
   },
   "source": [
    "# AI4Health Summer School - Pratical Session on Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiug6hc9k8bU"
   },
   "source": [
    "**Authors**: Hugues Roy, Maëlys Solal & Ninon Burgos\n",
    "\n",
    "For any comments or recommendation please contact: hugues.roy@inria.fr\n",
    "\n",
    "Many thanks to Charlotte Godard, Manon Heffernan & Swann Ruyter for their valuable feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U06C8v-mig98",
    "lines_to_next_cell": 0
   },
   "source": [
    "**Note**: If running in Colab, before starting, remember to change the runtime type to have access to GPU ressources: Runtime->Change Runtime Type, then choose GPU as hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/HuguesRoy/AI4Health_TP_diff.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Awjzl6oKZZfR"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U einops matplotlib tqdm torchtyping\n",
    "\n",
    "import math\n",
    "import os\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from typing import List, Optional, Tuple, Union, Callable, Dict\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from IPython.display import HTML\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from skimage.metrics import structural_similarity, mean_squared_error\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchtyping import TensorType\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from AI4Health_TP_diff.data import BraTSDataset\n",
    "from AI4Health_TP_diff.modules import (\n",
    "    Attention,\n",
    "    DiffusionModel,\n",
    "    Downsample,\n",
    "    LinearAttention,\n",
    "    PreNorm,\n",
    "    Residual,\n",
    "    Upsample,\n",
    "    WeightStandardizedConv2d,\n",
    "    default,\n",
    "    exists,\n",
    ")\n",
    "from AI4Health_TP_diff.noise import rand_perlin_2d_octaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wra0YxD-WB1R"
   },
   "source": [
    "<a id=\"Introduction\"></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4a4gL-xig9-"
   },
   "source": [
    "In this lab, we will investigate diffusion models, and specifically, denoising diffusion probabilistic models (DDPM) as presented by [Ho et al, 2020](https://arxiv.org/abs/2006.11239).\n",
    "\n",
    "We will focus on **image synthesis**, in particular, the synthesis of **T1-weighted magnetic resonance (MR) images** from isotropic Gaussian noise, and apply these methods for anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Mathematical formulation of diffusion models](#maths)\n",
    "2. [Dataset manipulation](#data)\n",
    "3. [Crash course on PyTorch (optional)](#pytorch)\n",
    "4. [Implementation of all components necessary for training a diffusion model](#implementation): \n",
    "    - (a) [U-Net and time embeddings](#unet), \n",
    "    - (b) [Beta schedule](#beta), \n",
    "    - (c) [Forward process](#forward),\n",
    "    - (d) [Reverse process (from different perspectives)](#reverse)\n",
    "5. [Application to anomaly detection with the anoDDPM model](#anomdetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxA5qtJhY6kC"
   },
   "source": [
    "<a id=\"maths\"></a>\n",
    "## 1. Mathematical formulation of diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I88Opm1M8y3j"
   },
   "source": [
    "### a. Big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YLLwXOWl7J-"
   },
   "source": [
    "A diffusion model is a type of generative model, it models the data distribution in a way that allows to generate new samples.\n",
    "As VAEs and GANs, it converts noise from a simple distribution (typically isotropic Gaussian) to a data sample.\n",
    "In the most simple case, we train a neural network that learns to gradually denoise data from pure noise.\n",
    "\n",
    "It can be represented in the following way: <img src='https://drive.google.com/uc?id=11mjeMdwiTiPiAYgXsagEKAVW2iTILs7b'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding the parameter $\\theta^*$ of a model distribution $p_\\theta(\\mathbf{x})$ that best approximates the true, unknown data distribution $ p_{\\text{data}} $. To formalize this objective, we need a criterion that reflects how well $p_\\theta$ aligns with the observed data. One widely-used method is **maximum likelihood estimation**, which involves maximizing the expected log-likelihood of the data under $p_\\theta$.\n",
    "$$\n",
    "\\theta^* = \\mathrm{arg max} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log p_{\\theta}(\\mathbf{x}) \\right]\n",
    "$$\n",
    "Here, the expectation is taken with respect to the empirical data distribution $p_{\\text{data}}$, representing the observed samples. By maximizing this quantity, we encourage the model $p_{\\theta}$​ to assign high probability to data drawn from the true distribution. In practice, it is not tractable, so we maximise the evidence lower bound (ELBO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJuZla7MOK96"
   },
   "source": [
    "It is a process consisting of two key components:\n",
    "\n",
    "1. **Fixed Forward Process $q$ (Noise Addition)**  \n",
    "   - Gradually adds noise to the input data according to a predefined **variance schedule**.\n",
    "   - Transforms the original data into pure noise over several timesteps.  \n",
    "\n",
    "2. **Learned Reverse Process $p_\\theta$ (Denoising)**  \n",
    "   - Aims to recover the original data from the noisy input.   \n",
    "   - This process is **learned by a neural network** (typically a U-Net), which predicts and removes the noise step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTTlGC9VmOTC"
   },
   "source": [
    "Both of these processes are indexed by time $t$, and are repeated a fixed number of times $T$.\n",
    "We start with $t=0$ and denote $\\mathbf{x}_0$ the real image from the data distribution.\n",
    "The forward process gradually samples noise that is added to the image at each timestep $t$.\n",
    "Given a sufficiently large $T$, and a good schedule for adding noise at each timestep, we should end up with an isotropic Gaussian distribution at $t=T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXZ2T9b2ORTi"
   },
   "source": [
    "### b. Forward process and beta schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCjnwAQtk7t3"
   },
   "source": [
    "Let $p_{\\text{data}}$ denote the real data distribution. We can sample from this distribution to obtain an image $\\mathbf{x}_0 \\sim p_{\\text{data}}$.\n",
    "\n",
    "The **forward diffusion process** progressively adds noise to this image at each timestep $t$. This noise is introduced according to a **beta schedule (or variance schedule)** $\\beta_t$, which determines the magnitude of noise added at each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWAWKhRJk2lI"
   },
   "source": [
    "#### Transition kernel: how to noise $\\mathbf{x}_{t-1}$ to obtain $\\mathbf{x}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTMPUGiwlKuy"
   },
   "source": [
    "Mathematically, this can be written with the following transition kernel, which defines how to go from $\\mathbf{x}_{t-1}$ to $\\mathbf{x}_t$ (by adding noise):\n",
    "$$q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta(t)} \\mathbf{x}_{t-1}, \\beta(t) \\mathit{\\boldsymbol{I}}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtL24MuqlHcM"
   },
   "source": [
    "#### Reparameterization trick\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOGgkNDXOT2F"
   },
   "source": [
    "The reparameterization trick allows to obtain a sample from $\\mathcal{N}(\\mu, \\sigma^2)$ by sampling from $\\mathcal{N}(0, 1)$. Indeed, to obtain $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we can sample $\\epsilon \\sim \\mathcal{N}(0, 1)$ and set $x = \\mu + \\sigma \\cdot \\epsilon$.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1hnnqq2oCJPrGIvZtY4sMfRVdbYXunNV1'>\n",
    "\n",
    "\n",
    "\n",
    "In our case, we can sample $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{\\epsilon}; \\mathbf{0}, \\mathit{\\boldsymbol{I}})$ and set\n",
    "$$\n",
    "\\mathbf{x}_t = \\sqrt{1 - \\beta(t)} \\mathbf{x}_{t-1} + \\beta(t) \\epsilon.\n",
    "$$\n",
    "\n",
    "For instance, we can obtain $x_1$ from $x_0$,\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1RnZPkrmq801dl1KpcGv8geZNpzUoL2T-'>\n",
    "\n",
    "and then $x_2$ from $x_1$,\n",
    "<img src='https://drive.google.com/uc?id=1EXKJY9pux9Fvzj6n9m7PGEyxCNd7CXU3'>\n",
    "\n",
    "which we can rewrite as:\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1i1zyoehX6mK7Z6Hrlqg-PJ9izeQaxvGr'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu_o9nJ2lO44"
   },
   "source": [
    "#### Sampling directly a noisy version of the data: obtaining $\\mathbf{x}_{t}$ from $\\mathbf{x}_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFcBZsbLlCjz"
   },
   "source": [
    "In practice, rather than explicitely adding noise step by step over multiple timesteps, we can directly sample a noisy version of the data $\\mathbf{x}i_t$ from $\\mathbf{x}_0$ using a closed-form solution:\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathit{\\boldsymbol{I}}),\n",
    "$$\n",
    "\n",
    "where $\\bar{\\alpha}_t $ is the **cumulative product** of $(1 - \\beta_s)$ up to timestep $t $:\n",
    "$$\n",
    "\\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_s).\n",
    "$$\n",
    "\n",
    "We also use the reparameterization trick to sample $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{\\epsilon}; \\mathbf{0}, \\mathit{\\boldsymbol{I}})$ and set  \n",
    "$$\n",
    "\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + (1 - \\bar{\\alpha}_t) \\epsilon.\n",
    "$$\n",
    "\n",
    "Schematically, we have:\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1txwVhPqoo3xo2pH8GfS4uskBrAWJUswM'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ll-EHWrvy3Bs"
   },
   "source": [
    "\n",
    "### c. Reverse process\n",
    "\n",
    "**The goal**\n",
    "\n",
    "We would like to reverse the forward diffusion process, and to do that, we use a neural network to approximate the reverse transitions at each timestep $t$, i.e. we learn to denoise the data at each timestep.\n",
    "Ideally, we would like to approximate all the transitions $p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+1})$ for each timestep $t$, so we would like to train a neural network to model the distribution $p_{\\theta}(\\mathbf{x}_t \\mid \\mathbf{x}_{t+1})$.\n",
    "\n",
    "**The challenge**\n",
    "\n",
    "Unfortunately, we don't have a direct access to $p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+1})$. During training, we only have access to: samples $\\mathbf{x}_0$ (from the training set), and the closed-form of the forward process (since it is fixed).\n",
    "\n",
    "This means that we are interested in learning the entire reverse chain, rather than only the transition from one step to another. In other words, we want consistency such as the output can be the input of the next step.\n",
    "\n",
    "\n",
    "**Objective: maximize the likelihood**\n",
    "\n",
    "\n",
    "We recall that as for any generative model, our objective is to maximize the log likelihood of the data distribution.\n",
    "In our case, we are trying to maximize the evidence lower bound (or ELBO), since attempting to maximize the log likelihood of the data distribution is intractable:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{x}_0) & \\geq \\mathbb{E}_{q(\\mathbf{x}_1, \\ldots, \\mathbf{x}_T|\\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_0,\\ldots,\\mathbf{x}_T)}{q(\\mathbf{x}_1,\\ldots,\\mathbf{x}_T | \\mathbf{x}_0)}\\right], \\quad \\text{(ELBO)}\\\\\n",
    "&= \\mathbb{E}_{q(\\mathbf{x}_1, \\ldots, \\mathbf{x}_T|\\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_T)\\prod_{t=1}^{T} p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{\\prod_{t=1}^{T} q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}\\right] \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Breaking down the loss**\n",
    "\n",
    "\n",
    "We can rewrite it as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} & = \\mathbb{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_0)} \\left[ \\log p_{\\theta} (\\mathbf{x}_{0}|\\mathbf{x}_1) \\right] \\quad (\\text{reconstruction term})\\\\\n",
    "& - \\sum_{t=1}^{T} \\mathbb{E}_{q(\\mathbf{x}_{t-1}, \\mathbf{x}_{t+1}|\\mathbf{x}_0)} \\left[ \\mathcal{D}_{KL}(q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}) || p_{\\theta}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1}))\\right] \\quad (\\text{consistency term}) \\\\\n",
    "& - \\mathbb{E}_{q(\\mathbf{x}_{t}|\\mathbf{x}_0)} \\left[ \\mathcal{D}_{KL}(q(\\mathbf{x}_{T}|\\mathbf{x}_{T-1}) || p_{\\theta}(\\mathbf{x}_{T}))\\right], \\quad (\\text{prior matching term}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "with $D_{KL}$ the Kullback Leibler divergence (it measures distance between distributions, and in our case, the distance between the model probability distribution and the true probability distribution).\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1RDq6nq7xMMcX4pJBl08aDQFMuiGcBHcE'>\n",
    "\n",
    "\n",
    "The following figure tries to illustrate the problem:\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1j2gleXfKM3PM9FhrFCEZiD2Zfyrxx77L'>\n",
    "\n",
    "Looking at the loss $\\mathcal{L}$ above, we notice the following components:\n",
    "- a **reconstruction term**, which simply measures how well the final step of the reverse process reconstructs a data point on average,\n",
    "- a **prior matching term**, where we fix the prior distribution $p_{\\theta}(\\mathbf{x}_T)$ to be standard isotropic Gaussian, it is equal  $q(\\mathbf{x}_T \\mid \\mathbf{x}_{T-1})$, we do not need to optimize it (since it is fixed), \n",
    "- a consistency term, which ensures that for each intermediate latent variable $\\mathbf{x}_t$, the reverse model distribution $p_{\\theta}(\\mathbf{x}_t \\mid \\mathbf{x}_{t+1})$ matches the forward noising process $q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})$, thereby making the forward and reverses processes consistent.\n",
    "It is this last consistency term that we must optimize.\n",
    "\n",
    "In the figure, we can see that we should optimize the salmon distribution to match the blue one.\n",
    "\n",
    "\n",
    "**Problem**\n",
    "\n",
    "\n",
    "However, this is difficult to train because it requires computing an expectation over two random variables associated with the data point. In the figure, we display only a single trajectory, but  imagine the number of possible trajectories, it is enormous! This makes it challenging to directly match these distributions during training.\n",
    "\n",
    "\n",
    "**The trick**\n",
    "\n",
    "\n",
    "To address this issue, we use a small trick: since the forward process is a Markov chain, we can inject additional information without changing the forward dynamics. Specifically, during training, we condition on the original image $\\mathbf{x}_0$, which we have access to:\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)\n",
    "$$\n",
    "\n",
    "\n",
    "Why? Because in the ELBO we can rewrite:\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_1,\\ldots,\\mathbf{x}_T | \\mathbf{x}_0) = q(\\mathbf{x}_T|\\mathbf{x}_0) \\prod_{t=2}^{T} q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0} )\n",
    "$$\n",
    "\n",
    "\n",
    "So that if we recompute the ELBO using the above equation:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{x}_0) & \\geq \\mathbb{E}_{q(\\mathbf{x}_1, \\ldots, \\mathbf{x}_T|\\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_0,\\ldots,\\mathbf{x}_T)}{q(\\mathbf{x}_1,\\ldots,\\mathbf{x}_T | \\mathbf{x}_0)}\\right], \\quad \\text{(ELBO)}\\\\\n",
    "&= \\mathbb{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_T)\\prod_{t=1}^{T} p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_T|\\mathbf{x}_0) \\prod_{t=2}^{T} q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0} )}\\right] \\\\\n",
    "& = \\sum_{t=0}^{T} \\mathcal{L}_t\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "So we have:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_0 & = \\mathbb{E}_{q(\\mathbf{x}_{1}|\\mathbf{x}_0)} \\left[ \\log p_{\\theta} (\\mathbf{x}_{0}|\\mathbf{x}_1) \\right] \\quad (\\text{reconstruction term})\\\\\n",
    "\\mathcal{L}_{t-1} & = -\\mathbb{E}_{q(\\mathbf{x}_{t}|\\mathbf{x}_0)} \\left[ \\mathcal{D}_{KL}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0} ) || p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}))\\right] \\quad (\\text{consistency term}) \\\\\n",
    "\\mathcal{L}_{T} & = \\mathbb{E}_{q(\\mathbf{x}_{t}|\\mathbf{x}_0)} \\left[ \\mathcal{D}_{KL}(q(\\mathbf{x}_{T}|\\mathbf{x}_{0}) || p_{\\theta}(\\mathbf{x}_{T})), \\quad (\\text{pior matching term}) \\right] \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "This lets us reformulate the optimization: instead of approximating $q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})$, we now aim to learn $p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)$ to match the true posterior $q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)$, which has a closed-form Gaussian expression that we will derive. The posterior describes how to denoise a noisy sample $\\mathbf{x}_t$ given access to the clean image $\\mathbf{x}_0$.\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1i0aJBnHAv5UWYsFNt1g-EguuyUTWKunU'>\n",
    "\n",
    "\n",
    "Now, the expectation only involves a single random variable! Great! Let's derive the closed-form expression of the posterior.\n",
    "\n",
    "\n",
    "**Closed-form of the posterior**\n",
    "\n",
    "\n",
    "Thanks to Bayes' rule, we can compute all the components required to derive this posterior. Bayes' rule gives:\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0) = \\frac{q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\, q(\\mathbf{x}_t \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}\n",
    "$$\n",
    "\n",
    "\n",
    "Rearranging terms, we obtain (because of the Markov chain: $q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)$) :\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) = \\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) \\, q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n",
    "$$\n",
    "\n",
    "\n",
    "After simplifying all the terms, we find that:\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\propto \\mathcal{N}(\\boldsymbol{\\mu}_q, \\mathbf{\\Sigma}_q)\n",
    "$$\n",
    "\n",
    "\n",
    "with:\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_q = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) \\mathbf{x}_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t) \\mathbf{x}_0}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma}_q = \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{I} = \\sigma_q^2 \\mathbf{I}\n",
    "$$\n",
    "\n",
    "\n",
    "We showed that $q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\propto \\mathcal{N}(\\boldsymbol{\\mu}_q, \\mathbf{\\Sigma}_q)$, so the reverse process is also Gaussian. Let's simplify the loss!\n",
    "\n",
    "\n",
    "**Simplifying the loss**\n",
    "\n",
    "We can now go back to the consistency term in the loss, and focus on $\\mathcal{D}_{KL}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0} ) || p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}))$ for a given timestep $t$.\n",
    "\n",
    "Since $q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0} )$ and $p_{\\theta}(\\mathbf{x}_{t-1}$ are both Gaussian, the KL divergence in the loss are simply the mean squared error between the two means weighted by the variance.\n",
    "\n",
    "\n",
    "$$\n",
    "D_{KL}(\\mathcal{N}(\\boldsymbol{\\mu}_q, \\mathbf{\\Sigma}_q) || \\mathcal{N}(\\boldsymbol{\\mu}_{\\theta}, \\mathbf{\\Sigma}_q)) = \\frac{1}{2 \\sigma_q} \\| \\boldsymbol{\\mu}_q - \\boldsymbol{\\mu}_{\\theta} \\|^{2}_{2},\n",
    "$$\n",
    "\n",
    "where we use the fact that $\\mathbf{\\Sigma}_q = \\sigma_q^2 \\mathbf{I}$. \n",
    "\n",
    "But if we look at $\\boldsymbol{\\mu}_q$\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_q = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) \\mathbf{x}_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t) \\mathbf{x}_0}{1 - \\bar{\\alpha}_t},\n",
    "$$\n",
    "\n",
    "\n",
    "we notice that everything except $\\mathbf{x}_0$ is known or given as input:\n",
    "- all $\\alpha_t$ coefficients are known (since they come from the variance schedule),\n",
    "- and $\\mathbf{x}_t$ is sampled using the forward $q( \\mathbf{x}_t | \\mathbf{x}_0)$.\n",
    "\n",
    "So we can rewrite the mean of the model distribution $p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_{t})$, and we see that the network is trying to predict the data point $\\mathbf{x}_0$, and we denote the prediction by $\\hat{\\mathbf{x}}_{\\theta}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_{\\theta} = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) \\mathbf{x}_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t) \\hat{\\mathbf{x}}_{\\theta}}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "\n",
    "**Data predictor loss**\n",
    "\n",
    "\n",
    "So substituting in the loss, we finally have:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t)}{2 \\sigma_q (1 - \\bar{\\alpha}_t)} \\mathbb{E}_{t,\\mathbf{x}_0,\\mathbf{x}_t} \\left[ \\| \\mathbf{x_0} - \\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t,t) \\|^2 \\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "It involves the mean squared error between the true data point and the prediction during training.​​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvP9MlsOP5E_"
   },
   "source": [
    "### d. Reverse process and U-Net model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtsD_YOAmAxv"
   },
   "source": [
    "The network can be used to predict either the data, denoted by $\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t,t)$, the noise, denoted by $\\hat{\\boldsymbol\\epsilon}_{\\theta}(\\mathbf{x}_t,t)$, or the score, given by $\\mathbf{s}_{\\theta}(\\mathbf{x}_t,t) $. The UNet takes as input the **noisy image** $\\mathbf{x}_t$ and the corresponding **timestep** $t$ (contrary to the UNet for segmentation that just takes the image).\n",
    "Note that the predicted output is a tensor of the same dimensions as the input image $\\mathbf{x}_t$.  \n",
    "\n",
    "In this lab session, we will use a **conditional U-Net**. The primary distinction between this architecture and a conventional U-Net is that the upsampling and downsampling blocks incorporate an additional timestep parameter in their forward pass.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZp_J61QZAnz"
   },
   "source": [
    "<a id=\"data\"></a>\n",
    "## 2. Dataset manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFPNEVEfev96"
   },
   "source": [
    "We will be using a brain imaging dataset called [IXI](https://brain-development.org/ixi-dataset/). This dataset contains nearly 600 MR images from normal, healthy subjects, with different MR acquisitions. We will focus on T1-weighted and T2-weighted MR images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wPDNzzijJ4B"
   },
   "source": [
    "### a. Data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuZHziafm2lb"
   },
   "source": [
    "The dataset can be found on this [server](https://aramislab.paris.inria.fr/files/data/databases/DL4MI/IXI-dataset.tar.gz) and alternatively in the following [GitHub repository](https://github.com/Easternwen/IXI-dataset).\n",
    "In the `size64` folder, there are 1154 files: 2 images for 577 subjects. The size of each image is (64, 64), i.e. a slice of the 3D image acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POvCf7gjig9_",
    "outputId": "b0ac118b-ab83-4a16-bd02-e25fd52b55db"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from GitHub\n",
    "! git clone https://github.com/Easternwen/IXI-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6S2PNSnjqTj"
   },
   "source": [
    "### b. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-EJlNSfig-A"
   },
   "source": [
    "There are two types of structural MR images: T1-weighted (T1w) images and T2-weighted (T2w) images.\n",
    "\n",
    "These imaging sequences do not highlight the same tissues: for example the cerebrospinal fluid (CSF)\n",
    "voxels are cancelled in T1w imaging whereas they are highlighted by\n",
    "the T2w imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "mTV0-ORtig-A",
    "outputId": "8eea1048-1349-4e62-9023-0dbde5dcdf50"
   },
   "outputs": [],
   "source": [
    "img_dir = \"./IXI-dataset/size64/\"\n",
    "\n",
    "sub_nb = \"013\"\n",
    "\n",
    "t1_path = os.path.join(img_dir, f\"sub-IXI{sub_nb} - T1.pt\")\n",
    "t2_path = os.path.join(img_dir, f\"sub-IXI{sub_nb} - T2.pt\")\n",
    "\n",
    "t1_img = torch.load(t1_path, weights_only=False)\n",
    "t2_img = torch.load(t2_path, weights_only=False)\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.swapaxes(t1_img, 0, 1), cmap=\"gray\", origin=\"lower\")\n",
    "plt.title(f\"T1 slice for subject {sub_nb}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.swapaxes(t2_img, 0, 1), cmap=\"gray\", origin=\"lower\")\n",
    "plt.title(f\"T2 slice for subject {sub_nb}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQoAEJ5XrWIQ"
   },
   "source": [
    "### c. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUDgi3mnrX8R"
   },
   "source": [
    "Pre-processing of neuroimaging data (and medical imaging data in general) is essential before doing any experiment and especially before training a neural network.\n",
    "It allows standardizing and improving the quality of the data, to ensure that the deep neural network can learn meaningful patterns and make accurate predictions.\n",
    "\n",
    "In this section, we go over common pre-processing steps for neuroimaging data for deep learning pipelines, and detail the pre-processing procedure for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-3xYWHwrbVU"
   },
   "source": [
    "#### Common pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7orAKygrcdC"
   },
   "source": [
    "**Registration**\n",
    "\n",
    "Registration consists of spatially aligning two or more images, either globally (rigid and affine registration) or locally (non-rigid registration), so that voxels in corresponding positions contain comparable information.\n",
    "It helps to standardize the neuroimaging data so that it is consistent across different subjects, scanners, and imaging modalities. This makes it easier for the deep neural network to learn patterns and make accurate predictions.\n",
    "\n",
    "**Bias field correction**\n",
    "\n",
    "MR images can be corrupted by a low frequency and smooth signal caused by magnetic field inhomogeneities. This bias field induces variations in the intensity of the same tissue in different locations of the image, which deteriorates the performance of image analysis algorithms such as registration.\n",
    "\n",
    "**Intensity normalization**\n",
    "\n",
    "Can help improve the performance of the deep neural network.\n",
    "\n",
    "**Cropping**\n",
    "\n",
    "Some specific regions of the registered images can be selected in order to remove the background and to reduce the computing power required when training deep learning models.\n",
    "\n",
    "\n",
    "**Motion correction and noise reduction**\n",
    "\n",
    "Can help to minimize sources of noise and improve the quality of the data.\n",
    "Neuroimaging data can be noisy due to a variety of factors, such as head motion, scanner artifacts, and biological variability.\n",
    "\n",
    "**Feature extraction**\n",
    "\n",
    "Pre-processing can be used to extract features from the neuroimaging data that are relevant to the task at hand.\n",
    "For example, if the goal is to classify brain regions based on their functional connectivity, pre-processing may involve computing correlation matrices from the fMRI time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyGhLEmGrpqE"
   },
   "source": [
    "#### Our pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi7NYpx0rT_1"
   },
   "source": [
    "In our case the data has already been pre-processed with the following steps:\n",
    "- both the T1 and T2 images have undergone bias field correction using N4BiasFieldCorrection,\n",
    "- the T1 image was affinely registered to a template (MNI space),\n",
    "- the T2 image was affinely registered to the T1 image in template space.\n",
    "\n",
    "Finally, 30 central axial 2D slices were extracted from the 3D images.\n",
    "\n",
    "Below, you will find images for each of these steps for subjects 084 and 184.\n",
    "<img src='https://drive.google.com/uc?id=1vCs9gRqqCzpR7QzhNABgelM7mES0dRue'>\n",
    "<img src='https://drive.google.com/uc?id=1w_Bd4NNCxWStqVLNFEg022XFwtmVyzqQ'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub_h0FjKkLc7"
   },
   "source": [
    "### d. Dataset class and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ09GYzRig-B"
   },
   "source": [
    "Let's create a custom `IXIDataset` class to easily access the data.\n",
    "\n",
    "We split the dataset between training and testings sets. The training set contains 80% of the images, and the testing set contains the remaining 20%.\n",
    "We will use the `train` or `test` mode of the `IXIDataset` class to access training or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZTzhiTgig-C",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class IXIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset utility class.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Path of the folder with all the images.\n",
    "        mode (str) {'train' or 'test'}:  Part of the dataset that is loaded.\n",
    "        transform (callable):  Optional transform to be applied on a sample.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir: str, mode: str = \"train\", transform: Callable = None):\n",
    "        files = sorted(os.listdir(img_dir))\n",
    "        patient_id = list(set([i.split()[0] for i in files]))\n",
    "\n",
    "        imgs = []\n",
    "\n",
    "        if mode == \"train\":\n",
    "            for i in patient_id[: int(0.8 * len(patient_id))]:\n",
    "                t1_path = os.path.join(img_dir, i + \" - T1.pt\")\n",
    "                t2_path = os.path.join(img_dir, i + \" - T2.pt\")\n",
    "                if os.path.isfile(t1_path) and os.path.isfile(t2_path):\n",
    "                    imgs.append((t1_path, t2_path))\n",
    "\n",
    "        elif mode == \"test\":\n",
    "            for i in patient_id[int(0.8 * len(patient_id)) :]:\n",
    "                t1_path = os.path.join(img_dir, i + \" - T1.pt\")\n",
    "                t2_path = os.path.join(img_dir, i + \" - T2.pt\")\n",
    "                if os.path.isfile(t1_path) and os.path.isfile(t2_path):\n",
    "                    imgs.append((t1_path, t2_path))\n",
    "\n",
    "        self.imgs = imgs\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        t1_path, t2_path = self.imgs[index]\n",
    "\n",
    "        t1 = torch.load(t1_path, weights_only=False)[None, :, :]\n",
    "        t2 = torch.load(t2_path, weights_only=False)[None, :, :]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            t1 = self.transform(t1)\n",
    "            t2 = self.transform(t2)\n",
    "\n",
    "        return {\"T1\": t1, \"T2\": t2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCcmPANyig-C"
   },
   "source": [
    "Using this class and the `DataLoader` class from `torch.utils.data`, we can\n",
    "easily access our dataset. Here is a quick example on how to use it:\n",
    "\n",
    "```python\n",
    "# Create a DataLoader instance for the training set\n",
    "dataloader = DataLoader(\n",
    "    IXIDataset(img_dir, mode=\"train\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# You will get a batch of samples from the training set\n",
    "for batch in dataloader:\n",
    "    # batch is a dictionary with two keys:\n",
    "    # - batch[\"T1\"] is a tensor with shape (batch_size, 64, 64) with the T1 images for the samples in this batch\n",
    "    # - batch[\"T2\"] is a tensor with shape (batch_size, 64, 64) with the T2 images for the samples in this batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV0Vb2X5Zp0P"
   },
   "source": [
    "<a id=\"pytorch\"></a>\n",
    "## 3. Crash course on neural networks with PyTorch (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lklFFbIKA5WS"
   },
   "source": [
    "### Multi-Layer Perceptron\n",
    "A multi-layer perceptron (MLP) is a simple type of feedforward artificial neural network that consists of multiple layers of neurons connected in a fully connected (or dense) manner.\n",
    "\n",
    "It allows to approximate any continuous function $f : \\mathbb{R}^m \\to \\mathbb{R}^o$, to map a set of features $X = (x_1, x_2, \\ldots, x_m) \\in \\mathbb{R}^m$ to a label $y \\in \\mathbb{R}^o$.\n",
    "\n",
    "Its key components include:\n",
    "- the input layer: receives the raw input features, and does not perform any computation,\n",
    "- the hidden layers: one or more layers of neurons, where each neuron computes a weighted sum of its inputs, adds a bias and applies a nonlinear activation function,\n",
    "- the output layer which produces the final layer, often by using a softmax activation for classification or linear activation for regression.\n",
    "\n",
    "Here is a schematic diagram:\n",
    "<img src='https://drive.google.com/uc?id=1kDmvTPFBnG2_X04bPbSHXnmda5eXJFGW'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19MddTXkst0O"
   },
   "source": [
    "In a neural network everything is called a layer although the operations performed in the layers may be very different. You will find below a summary of some of the different operations that may be performed in a neural network (and more specifically, a convolutional neural network).\n",
    "\n",
    "As medical images are often 3D, we introduce the operations performed by the neural network layers in the 3D case.\n",
    "In the implementation, we will use their 2D counterparts since our images are 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt_5UpLIiTzM"
   },
   "source": [
    "### Feature maps\n",
    "\n",
    "The outputs of the layers in a convolutional neural network are called feature maps. In the case of 3D images, their size is written with the format `n_channels @ dim1 x dim2 x dim3`, and in the case of 2D images (our case) `n_channels @ dim1 x dim2`.\n",
    "\n",
    "For a 2D CNN the dimension of the feature maps is actually 4D as the first dimension is the batch size. This dimension is added by the `DataLoader` of pytorch which stacks the 3D tensors computed by a `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1VVZU27iXCR",
    "outputId": "72672648-6688-4502-d6ca-7482c12fdd73"
   },
   "outputs": [],
   "source": [
    "img_dir = \"./IXI-dataset/size64/\"\n",
    "batch_size = 1\n",
    "\n",
    "dataset = IXIDataset(img_dir, mode=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "data = next(iter(dataloader))\n",
    "\n",
    "print(\"Shape of IXIDataset output:\", dataset[0][\"T1\"].shape)\n",
    "print(\"Shape of DataLoader output:\", data[\"T1\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK_IkxwliGve"
   },
   "source": [
    "\n",
    "### 3D convolutions (`nn.Conv3d`)\n",
    "\n",
    "[Link to the PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html)\n",
    "\n",
    "The main arguments of this layer are the input channels (`in_channels`), the output channels (number of filters trained) (`out_channels`) and the size of the filter (or kernel) (`kernel_size`).\n",
    "\n",
    "For the `kernel_size` parameter, if an integer `k` is given the kernel will be a cube of size `k * k * k`. It is possible to construct rectangular kernels by entering a tuple (but this is very rare).\n",
    "\n",
    "You will find below an illustration of how a single filter produces its output feature map by parsing one feature map. The size of the output feature map produced depends of the convolution parameters and can be computed with the following formula:\n",
    "\n",
    "$$O_i = \\frac{I_i-k+2P}{S} + 1$$\n",
    "\n",
    "*   $O_i$ the size of the output along the ith dimension\n",
    "*   $I_i$ the size of the input along the ith dimension\n",
    "*   $k$ the size of the kernel\n",
    "*   $P$ the padding value\n",
    "*   $S$ the stride value\n",
    "\n",
    "In the following example $\\frac{5-3+2*0}{1}+1 = 3$\n",
    "\n",
    "![2D convolutional layer gif](https://drive.google.com/uc?id=166EuqiwIZkKPMOlVzA-v5WemJE2tDCES)\n",
    "\n",
    "To be able to parse all the feature maps of the input, a filter is actually a 4D tensor of size `(input_channels, k, k, k)`. The set of all filters included in a convolutional layer is then a 5D tensor stacking all the filters of size `(output_channels, input_channels, k, k, k)`.\n",
    "\n",
    "Each filter is also associated to a bias value that is a scalar added to all the feature maps it produces. Then the bias is a 1D vector of size `output_channels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETjl7kp17-IM",
    "outputId": "54cc6fe3-e9f3-4652-c9ab-b1c1e099314a"
   },
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv3d(8, 16, 3)\n",
    "print(\"Weights shape:\", conv_layer.weight.shape)\n",
    "print(\"Bias shape:\", conv_layer.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0ItmAuXBM9P"
   },
   "source": [
    "### Weight Standardized Convolution (WeightedStandardizedConv2d, WSConv)\n",
    "\n",
    "Weight Standardized Convolution is a technique introduced in [(Qiao et al., 2019)](https://arxiv.org/pdf/1903.10520) to improve the training stability of convolutional neural networks, especially when batch sizes are small.\n",
    "\n",
    "The core idea is to normalize the kernel weights of the convolution filters (before performing the convolution operation), rather than the activations.\n",
    "It leads to more stable gradients, especially in small-batch settings, which llows stabilizing training.\n",
    "It also works well with GroupNorm (that we evoke later in the tutorial), and prevents weight explosion or collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_conv_layer = WeightStandardizedConv2d(8, 16, 3)\n",
    "print(\"Weights shape:\", weighted_conv_layer.weight.shape)\n",
    "print(\"Bias shape:\", weighted_conv_layer.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMmdLHtSA9tZ"
   },
   "source": [
    "### Batch Normalization (`nn.BatchNorm3d`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d)\n",
    "\n",
    "Learns to normalize feature maps according to [(Ioffe & Szegedy, 2015)](https://arxiv.org/abs/1502.03167). The following formula is applied on each feature map  $FM_i$:\n",
    "\n",
    "$$FM^{normalized}_i = \\frac{FM_i - mean(FM_i)}{\\sqrt{var(FM_i) + \\epsilon}} * \\gamma_i + \\beta_i$$\n",
    "\n",
    "*   $\\epsilon$ is a hyperparameter of the layer (default=1e-05)\n",
    "*   $\\gamma_i$ is the value of the scale for the ith channel (learnable parameter)\n",
    "*   $\\beta_i$ is the value of the shift for the ith channel (learnable parameter)\n",
    "\n",
    "This layer does not have the same behaviour during training and evaluation, this is why it is needed to put the model in evaluation mode in the test function with the command `.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QPASZehDHjc",
    "outputId": "daa2d4f7-9cb7-4c04-a14a-dda87a6ce036"
   },
   "outputs": [],
   "source": [
    "batch_layer = nn.BatchNorm3d(16)\n",
    "print(\"Gamma value:\", batch_layer.state_dict()[\"weight\"].shape)\n",
    "print(\"Beta value:\", batch_layer.state_dict()[\"bias\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXW9iQkeQsRz"
   },
   "source": [
    "### Group Normalization (nn.GroupNorm)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html)\n",
    "\n",
    "Applies group normalization over a mini-batch of inputs, as described in the paper [(Wu & He, 2018)](https://arxiv.org/abs/1803.08494).\n",
    "The following formula is applied:\n",
    "$$ y = \\frac{x - \\mathbb{E}(x)}{\\sqrt{\\mathbb{V}(x)+\\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "The layer takes as parameters:\n",
    "* `num_groups (int)` the number of groups to separate the channels into\n",
    "* `num_channels (int)` the number of channels expected in input\n",
    "* `eps (float)` a hyperparameter for numerical stability (default=1e-05)\n",
    "* `affine (bool)` a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases), denoted \\gamma and \\beta in the above formula\n",
    "\n",
    "The channels (or feature maps) are divided into smaller groups and the features within each group are normalized.\n",
    "\n",
    "Unlike with BatchNorm where normalization is done across examples in a batch, in GroupNorm, the normalization is done separately for each example in a batch, meaning that the normalization is independent of the batch size.\n",
    "This is particularly useful when working with small or variable batch sizes, which is common in diffusion models.\n",
    "Here is a schematic drawing of different types of normalizations:\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1FzBtm94yF5_uz4yV9TNla1rwuFGr1yeP'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_norm_layer = nn.GroupNorm(2, 2)\n",
    "print(\"Gamma value:\", group_norm_layer.state_dict()[\"weight\"].shape)\n",
    "print(\"Beta value:\", group_norm_layer.state_dict()[\"bias\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4TxeR3lEZeI"
   },
   "source": [
    "### Activation function (`nn.LeakyReLU`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)\n",
    "\n",
    "In order to introduce non-linearity in the model, an activation function is introduced after the convolutions. It is applied on all intensities independently.\n",
    "\n",
    "The graph of the Leaky ReLU is displayed below, $\\alpha$ being a hyperparameter of the layer (`default=0.01`):\n",
    "\n",
    "![Leaky ReLU graph](https://sefiks.com/wp-content/uploads/2018/02/prelu.jpg?w=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dRn0vyuBXLc"
   },
   "source": [
    "### Another activation function (nn.SiLU)\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html)\n",
    "\n",
    "Another type of non-linear activation function is the SiLU function, or Sigmoid Linear Unit, also known as the swish function:\n",
    "$$ \\text{silu}(x) = x * \\sigma(x)$$\n",
    "where $\\sigma(x) : x \\mapsto \\frac{1}{1 + e^{-x}}$ is the logistic sigmoid.\n",
    "\n",
    "Compared to the Leaky ReLU, the SiLU is smooth and differentable everywhere, allowing for better optimization during training, especialy in deeper networks.\n",
    "Its monotonicity allows capturing more complex relationships.\n",
    "However, it is more expensive to compute.\n",
    "\n",
    "The graph of the SiLU is displayed below:\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1b4U2peBgMTOgLGu6idw0ruh2qbJAZd1b'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3Oflm0QGHHG"
   },
   "source": [
    "### Pooling function (`nn.MaxPool2d`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html)\n",
    "\n",
    "The structure of the pooling layer is very similar to the convolutional layer: a kernel is passed through the input with a defined size and stride. However there is no learnable parameters in this layer, the kernel outputs the maximum value of the part of the feature map it covers.\n",
    "\n",
    "Here is an example in 2D of the standard layer of pytorch `nn.MaxPool2d`:\n",
    "\n",
    "![nn.MaxPool2d behaviour](https://drive.google.com/uc?id=1qh9M9r9mfpZeSD1VjOGQAl8zWqBLmcKz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4Py8zLGX2vk"
   },
   "source": [
    "### Flatten (`nn.Flatten`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
    "\n",
    "This layer flattens the array to a 1D array. This operation is necessary between the 3D (or 2D) convolutions and the fully-connected layers.\n",
    "\n",
    "You also can perform the flatten operation in the `forward` method of the network with `view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9ctrE3_X4jH",
    "outputId": "d436fa75-699a-446f-91a1-5af847a2f97d"
   },
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "input_tensor = torch.rand(8, 16, 4, 5, 4)\n",
    "output_tensor = flatten(input_tensor)\n",
    "\n",
    "print(\"Shape of the output tensor:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-65jRniEjr2"
   },
   "source": [
    "### Dropout (`nn.Dropout`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "\n",
    "The aim of a dropout layer is to replace a fixed proportion of the input values by 0 during training only. This has proven to be an effective technique for regularization.\n",
    "\n",
    "This layer does not have the same behaviour during training and evaluation, this is why it is needed to put the model in evaluation mode in the test function with the command `.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0O3cCR7HGge",
    "outputId": "a326d142-0fb1-4e54-daff-a49ae7256551"
   },
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.5)\n",
    "input_tensor = torch.rand(10)\n",
    "output_tensor = dropout(input_tensor)\n",
    "print(\"Input:\", input_tensor)\n",
    "print(\"Output:\", output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hL2TdAnE-jn"
   },
   "source": [
    "### Fully-Connected Layers (`nn.Linear`)\n",
    "\n",
    "[PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "\n",
    "The fully connected (FC) layers take as input 2D vectors of size `(batch_size, N)`. They have two mandatory arguments, the number of values per batch of the input and the number of values per batch of the output.\n",
    "\n",
    "Each output neuron in a FC layer is a linear combination of the inputs + a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "um2OupjmGy94",
    "outputId": "42df5073-588e-473d-f339-f358d52ff6a7"
   },
   "outputs": [],
   "source": [
    "fc = nn.Linear(16, 2)\n",
    "print(\"Weights shape:\", fc.weight.shape)\n",
    "print(\"Bias shape:\", fc.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuRPuTmkig-C"
   },
   "source": [
    "<a id=\"implementation\"></a>\n",
    "## 4. Implementation of the diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEceqPi4bLCw"
   },
   "source": [
    "<a id=\"unet\"></a>\n",
    "### a. U-Net conditioning & time embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcHnjAyHSFze"
   },
   "source": [
    "#### Time Positional Embedding\n",
    "\n",
    "In this section, we explore how to represent timesteps in a neural network using **positional embeddings**.  \n",
    "\n",
    "**Why not use raw time values directly?**  \n",
    "\n",
    "Suppose we want to encode $ T = 500 $ timesteps. A straightforward approach might be to pass the raw timestep value $t$ directly into the network. However, this approach has several drawbacks:  \n",
    "\n",
    "- **Numerical instability**: Large values of  $t$ can introduce numerical instability, making it harder for the model to learn meaningful patterns.  \n",
    "- **Inconsistent scaling**: Normalizing time values (e.g., to a range between 0 and 1) can be problematic. Different sequences of varying lengths would be normalized differently, making it difficult for the model to generalize across sequences of different durations.  \n",
    "\n",
    "To address these issues, we use **time positional encoding**, which provides a unique representation for each timestep.  \n",
    "\n",
    "\n",
    "**Sinusoidal Positional Embedding**  \n",
    "\n",
    "Positional encoding assigns a distinct vector representation (embedding) to each timestep $t \\in [0,T]$, ensuring that no two timesteps share the same embedding. The classical **sinusoidal position embedding** is defined as follows:  \n",
    "\n",
    "$$\n",
    "P(t, 2i) = \\sin \\left( \\frac{t}{n^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(t, 2i+1) = \\cos \\left( \\frac{t}{n^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $0 \\leq i < d/2 $ maps the column indices,  \n",
    "- $d$ is the embedding dimension,  \n",
    "- $n$ is a user-defined scalar.  \n",
    "\n",
    "**Key Observations**  \n",
    "\n",
    "- The **sine function** is applied for the even-indexed columns, whereas the **cosine function** is applied for the odd-indexed columns.  \n",
    "- This method ensures that each timestep is uniquely represented in the range $[-1,1]$.  \n",
    "- The sinusoidal positional encoding introduces periodic patterns, which help the model capture **temporal dependencies** effectively. It maintains relative distance between positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJWC8mBULxf2"
   },
   "source": [
    "[**TODO**]: complete the sinusoidal embedding\n",
    "\n",
    "As it is going to be passed trough a MLP, we can concatenate them directly. So we have:\n",
    "\n",
    "$$\n",
    "P(t, i) = \\sin \\left( \\frac{t}{n^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(t, \\frac{d}{2} + i) = \\cos \\left( \\frac{t}{n^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $0 \\leq i < d/2 $ maps the column indices,  \n",
    "- $d$ is the embedding dimension,  \n",
    "- $n$ is a user-defined scalar.\n",
    "\n",
    "\n",
    "\n",
    "We will use the following mathematical trick to improve numerical stability and efficiency:\n",
    "\n",
    "$$\n",
    "n^{\\frac{2i}{d}} = \\exp \\left( \\frac{2i}{d} \\log(n)\\right)\n",
    "$$\n",
    "\n",
    "and set $n = 10000 $ (it is related to the wavelength of the sinusoidal functions, the distance between the peaks of the sinusoidal functions increases exponentially as the position in the sequence increases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : we can use the function:\n",
    "- `torch.arange`: similar to numpy arange function ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.arange.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyNyCy7vxI57"
   },
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time: torch.Tensor):\n",
    "        # naive implementation\n",
    "\n",
    "        # TODO\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "\n",
    "        # HERE for the representation we want\n",
    "        # first half columns to be the sine function\n",
    "        # second half colmuns to be the cosine function\n",
    "\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtoJu0pFgshN"
   },
   "source": [
    "Here we plot an example for the timestep 10. We can see the associated embbeding (vector representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "DJc5zQBIWeDg",
    "outputId": "44e7f425-2671-4e71-b8a1-17fa38b05554"
   },
   "outputs": [],
   "source": [
    "sinusoidal_embedding = SinusoidalPositionEmbeddings(dim=64)\n",
    "n = 50\n",
    "times = torch.tensor([10])\n",
    "\n",
    "times_embedded = sinusoidal_embedding(times)\n",
    "\n",
    "plt.imshow(times_embedded.detach().numpy(), vmin=-1.0, vmax=1.0, interpolation=\"none\")\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.ylabel(\"time\")\n",
    "plt.yticks([0], [10], rotation=\"vertical\")\n",
    "plt.xlabel(\"embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMZwOw2IhBNR"
   },
   "source": [
    "Here we plot the embeddings for different timesteps, and obtain a matrix. We can see the first half of the matrix corresponds to the sine function and the second half to the cosine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "vJRR5h_CSW_M",
    "outputId": "c5f336c8-bbf6-40a1-9ce9-58d17fa5e2b6"
   },
   "outputs": [],
   "source": [
    "n = 500\n",
    "timesteps = torch.arange(0, n)\n",
    "\n",
    "sinusoidal_embedding = SinusoidalPositionEmbeddings(dim=64)\n",
    "times_embedded = sinusoidal_embedding(timesteps)\n",
    "\n",
    "plt.imshow(\n",
    "    times_embedded.detach().numpy(),\n",
    "    vmin=-1.0,\n",
    "    vmax=1.0,\n",
    "    interpolation=\"none\",\n",
    "    aspect=0.1,\n",
    ")\n",
    "plt.xlabel(\"embedding\")\n",
    "plt.ylabel(\"time\")\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Vu0dI46hUnn"
   },
   "source": [
    "#### U-Net conditioning  \n",
    "\n",
    "Previously, we defined the **time positional embedding**, which maps a given timestep to a vector representation. However, how exactly should this embedding be incorporated into the U-Net?\n",
    "\n",
    "Remember that we would like a **reparameterization** of the network for each timestep.\n",
    "\n",
    "**Why Not Use Time Embedding Only at the Input Layer?**  \n",
    "\n",
    "One possible approach is to provide the time embedding **only at the beginning of the network**. However, this method has significant drawbacks:  \n",
    "\n",
    "- In **deep architectures**, the influence of the time embedding may **diminish** as information propagates through the network.  \n",
    "- The network may struggle to **retain** temporal information, reducing its ability to adapt effectively to different timesteps.  \n",
    "\n",
    "**A Better Approach: Conditioning Each U-Net Block**  \n",
    "\n",
    "To ensure that the time embedding remains influential throughout the network, we **condition each block** of the U-Net with the time embedding. This technique effectively **reparameterizes** the network for each timestep, ensuring that temporal information is **consistently and strongly integrated** at every stage.  \n",
    "\n",
    "By applying this conditioning mechanism:  \n",
    "- Each U-Net block can **adapt its behavior** based on the current timestep.  \n",
    "- The model can learn more expressive **time-dependent features**, improving its ability to process noisy images at different timesteps.  \n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1oAir3Y9wE64yjxKQP3DFK12JHrk229e3'>\n",
    "\n",
    "**Scope of This Section**  \n",
    "\n",
    "In this section, we will not dive deeply into the **U-Net architecture itself**. Instead, we will focus specifically on how the **time embedding is incorporated** into the network.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7limK5a4-jNh"
   },
   "source": [
    "We will use the following class called `Block` to pass the information of the UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njyheuhcgML0"
   },
   "source": [
    "In diffusion models, a ResNet block is typically composed of multiple layers to apply time conditioning via **scale-shift normalization** (also called FiLM: feature-wise Linear Modulation ([Perez et al., 2017](https://arxiv.org/abs/1709.07871))).\n",
    "\n",
    "The following figure shows the architecture:\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1W48p9bkS5biMBcCWnOJ6wZmp66O6mWdl' width='700'>\n",
    "\n",
    "and below is a detailed description of each layer:\n",
    "\n",
    "1. **Initial convolution**\n",
    "\n",
    "  A standardized convolution is applied to the input to project it into the desired feature space:\n",
    "\n",
    "  $$\n",
    "  x_1 = \\mathrm{Conv}(x_0)\n",
    "  $$\n",
    "\n",
    "2.  **Normalization**\n",
    "\n",
    "  The input tensor $x$ is normalized using a method such as GroupNorm:\n",
    "\n",
    "  $$\n",
    "  x_2 = \\mathrm{Norm}(x_1)\n",
    "  $$\n",
    "\n",
    "3. **Scale-Shift Modulation**\n",
    "\n",
    "   A time embedding $t$ is passed through a small MLP $\\Phi$ to produce scaling and shifting parameters $\\gamma$ and $\\beta$, applied channel-wise:\n",
    "\n",
    "   $$\n",
    "   (\\gamma, \\beta) = \\Phi(t)\n",
    "   $$\n",
    "\n",
    "   These parameters are applied channel-wise to the normalized features:\n",
    "\n",
    "   $$\n",
    "   x_3 = (1 + \\gamma) \\cdot x_2 + \\beta\n",
    "   $$\n",
    "\n",
    "   It is important that the normalization is done prior to the scale-shift modulation, so that the scale-shift modulation has an effect.\n",
    "\n",
    "4. **Activation**\n",
    "\n",
    "   A non-linear activation function (e.g., SiLU or GELU) is applied:\n",
    "\n",
    "   $$\n",
    "   x_4 = \\mathrm{Activation}(x_3)\n",
    "   $$\n",
    "\n",
    "   It comes after the conditioning, to ensure that the activation accounts for the timestep.\n",
    "\n",
    "5. **Convolution + Normalization + Activation**\n",
    "\n",
    "   A convolutional layer is applied to propagate local spatial information:\n",
    "\n",
    "   $$\n",
    "   x_7 = \\mathrm{Activation}(\\mathrm{Norm}(\\mathrm{Conv}(x_4)))\n",
    "   $$\n",
    "\n",
    "6. **Residual Connection**\n",
    "\n",
    "   The original input is added back to the transformed output. If the number of channels has changed, a linear projection $h(x_0)$ (typically a $1 \\times 1$ convolution) is used to align dimensions:\n",
    "\n",
    "   $$\n",
    "   \\text{output} = h(x_0) + x_7\n",
    "   \\quad \\text{or} \\quad\n",
    "   \\text{output} = x_0 + x_7\n",
    "   $$\n",
    "\n",
    "   The residual connection is important because it helps preserve the structure of the data from one step to the next.\n",
    "   Moreover, it allows the network to condition its internal computations on time.\n",
    "   Specifically, it combines:\n",
    "\n",
    "   - the original input (processed without time conditioning), and\n",
    "   - the transformed features (modulated by the time embedding).\n",
    "\n",
    "   This combination allows the block to add time-dependent refinements without discarding the essential spatial or semantic information in the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : we can use the functions:\n",
    "- `nn.GroupNorm`: torch module that applies group normalization ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html))\n",
    "- `nn.SiLU`: torch module that applies the SiLU activation function ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html))\n",
    "- `WeightStandardizedConv2d` : our custom module that applies weight standardization to a convolutional layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De5NHQdG-WQF"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim: int, dim_out: int, groups: int = 8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        scale_shift: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if scale_shift is not None:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : you can use the block we defined before and:\n",
    "- `.chunk()` method : Attempts to split a tensor into the specified number of chunks. Each chunk is a view of the input tensor ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk))\n",
    "- `nn.Conv2D`: torch module to apply 2D Convolution ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, dim_out: int, time_emb_dim: int, groups: int = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time_emb: Optional[torch.Tensor] = None):\n",
    "        # TODO: implement the forward pass of the ResnetBlock\n",
    "\n",
    "        scale_shift = None\n",
    "\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            # TODO\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            # time_emb = time_emb.view(time_emb.shape[0], time_emb.shape[1], 1, 1)\n",
    "\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unet**: The Unet architecture is <span style=\"color:red\">given</span>, it incorporates the resnet structure that was defined earlier.\n",
    "\n",
    "\n",
    "This model is a hierarchical encoder-decoder (UNet) designed for diffusion models. It uses skip connections, attention, and time conditioning via sinusoidal embeddings. Here is a breakdown of each key component:\n",
    "\n",
    "1. **Stem**: A 1×1 convolution expands the number of channels in the input image. We also keep a copy of this high-resolution feature map to use later for the final skip connection.\n",
    "\n",
    "2. **Time embedding**: We use sinusoidal positional embeddings to encode the timestep information. These embeddings are passed through an MLP, producing a time vector of shape `(B, time_dim)`. This vector modulates all residual blocks via FiLM (Feature-wise Linear Modulation), injecting temporal information.\n",
    "\n",
    "3. **Encoder**: The encoder reduces spatial resolution while increasing the number of channels. At each level we have: two ResNet blocks with time conditioning, a linear Attention layer and a downsampling operation. Intermediate activations are saved in a stack `h` for later use in the decoder.\n",
    "\n",
    "4. **Bottleneck**: At the lowest spatial resolution: One ResNet block -> Attention -> ResNet block. This is the most compressed representation of the input.\n",
    "\n",
    "5. **Decoder**: The decoder progressively restores the spatial resolution. At each level: two skip connections from the encoder are concatenated, two time-conditioned ResNet blocks, one linear attention layer and an upsampling operation (except at the top level).\n",
    "\n",
    "6. **Output layer**: The final decoder feature map is merged with the high-res skip from the stem. One last ResNet block refines the output. A 1x1 convolution to project to the desired number of output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTzMRqSPh81l"
   },
   "outputs": [],
   "source": [
    "class Unet(DiffusionModel):\n",
    "    \"\"\"\n",
    "    Hierarchical encoder-decoder with skip connections + attention,\n",
    "    conditioned on a sinusoidal time embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        init_dim: Optional[int] = None,\n",
    "        out_dim: Optional[int] = None,\n",
    "        dim_mults: Optional[Tuple[int, int, int]] = (1, 1, 2),\n",
    "        channels: Optional[int] = 1,\n",
    "        resnet_block_groups: Optional[int] = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        input_channels = channels\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(\n",
    "            input_channels, init_dim, 1, padding=0\n",
    "        )  # changed to 1 and 0 from 7,3\n",
    "\n",
    "        # ---------- resolution schedule ---------------------------------------------\n",
    "        # dims = [64, 64, 64, 128]  for dim=64, dim_mults=(1,1,2)\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # ResNet block template with fixed GroupNorm groups\n",
    "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # ---------- time embedding MLP ----------------------------------------------\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # ---------- encoder (downs) layers ------------------------------------------\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Downsample(dim_in, dim_out)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ---------- bottleneck ------------------------------------------------------\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        # ---------- decoder (ups) ---------------------------------------------------\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ---------- decoder (ups) ---------------------------------------------------\n",
    "        self.out_dim = default(out_dim, channels)\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor):\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        r = x.clone()\n",
    "\n",
    "        # -------- time conditioning -------------------------------------------------\n",
    "        t = self.time_mlp(time)\n",
    "        h = []\n",
    "\n",
    "        # -------- encoder -----------------------------------------------------------\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        # -------- bottleneck --------------------------------------------------------\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        # -------- decoder -----------------------------------------------------------\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        # -------- final head ---------------------------------------------------------\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXy0a8OqIY8C"
   },
   "source": [
    "<a id=\"beta\"></a>\n",
    "### b. Beta schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAjdtx3hWxp4"
   },
   "source": [
    "\n",
    "The forward diffusion process progressively adds noise to an image. This noise is introduced according to a known **variance schedule (or beta schedule)** $\\beta_t$, which determines the magnitude of noise added at each timestep $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSXWFymCKNcV"
   },
   "source": [
    "#### Types of beta schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu6TLD6QKYzi"
   },
   "source": [
    "The choice of the beta schedule significantly impacts the model's performance. Common schedules include:  \n",
    "- **Linear beta schedule**  \n",
    "- **Cosine beta schedule**  \n",
    "- **Quadratic beta schedule**  \n",
    "- **Sigmoid beta schedule**  \n",
    "\n",
    "While it is possible to define a custom beta schedule, in general, it is important to ensure that the noise follows the assumptions required for **sampling from an isotropic Gaussian**, particularly for $p_T$, the final step of the diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jw2zpZGVAIC"
   },
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps: int, s: float = 0.008) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (T).\n",
    "        s (float): Offset.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (timesteps,) containing the beta values.\n",
    "\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the sigmoid beta schedule\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (T).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A tensor of shape (timesteps,) containing the beta values.\n",
    "\n",
    "    \"\"\"\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-udjlGRVKbH8"
   },
   "source": [
    "#### Implementation of a quadratic beta schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE5ispCEK0dQ"
   },
   "source": [
    "Although referred to as \"quadratic,\" this schedule does not perform a true quadratic interpolation. Instead, it is defined as:\n",
    "\n",
    "$$\n",
    "\\beta_t = \\left(\\sqrt{\\beta_{\\text{start}}} + \\left(\\sqrt{\\beta_{\\text{end}}} - \\sqrt{\\beta_{\\text{start}}}\\right) \\cdot \\frac{t}{T}\\right)^2\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $\\beta_{\\text{start}} $ is the initial beta value,  \n",
    "- $\\beta_{\\text{end}} $ is the final beta value,\n",
    "- $ T $ is the total number of timesteps,  \n",
    "- $ t $ is the current timestep $ t\\in [0, T]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : we can use the function:\n",
    "- `torch.linspace`: torch implementation similar to numpy linspace ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.linspace.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6D4Y4y1TE78Q"
   },
   "outputs": [],
   "source": [
    "# TODO implement the quadratic beta schedule.\n",
    "\n",
    "\n",
    "def quadratic_beta_schedule(timesteps: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the quadratic beta schedule.\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (T).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (timesteps,) containing the beta values.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2FWfP6lKftm"
   },
   "source": [
    "#### Implementation of a linear beta schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : we can use the function:\n",
    "- `torch.linspace`: torch implementation similar to numpy linspace ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.linspace.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yO9yw2zLTKD"
   },
   "outputs": [],
   "source": [
    "# TODO implement the linear beta schedule\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the linear beta schedule.\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (T).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (timesteps,) containing the beta values.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkKxEN7XNFr4"
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNX9-j8j-2dQ"
   },
   "outputs": [],
   "source": [
    "timesteps = 1001\n",
    "\n",
    "schedules = {\n",
    "    \"quadratic\": quadratic_beta_schedule,\n",
    "    \"linear\": linear_beta_schedule,\n",
    "    \"sigmoid\": sigmoid_beta_schedule,\n",
    "    \"cosine\": cosine_beta_schedule,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, len(schedules), figsize=(20, 5), sharex=True, sharey=False)\n",
    "\n",
    "for i, (name, schedule) in enumerate(schedules.items()):\n",
    "    beta = schedule(timesteps)\n",
    "    ax[i].plot(beta, label=name)\n",
    "    ax[i].set_title(name)\n",
    "    ax[i].set_xlabel(\"timestep\")\n",
    "\n",
    "ax[0].set_ylabel(\"beta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAq9dhwiNSJe"
   },
   "source": [
    "#### Key takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx_vvWWkPam3"
   },
   "source": [
    "- The quadratic beta schedule ensures a smooth progression of noise variance over time.  \n",
    "- The interpolation is performed in **square-root space**, ensuring better control over noise scaling.  \n",
    "- This schedule prevents excessive noise in the early steps while maintaining enough variance for effective learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwAw1srRNydD"
   },
   "source": [
    "<a id=\"forward\"></a>\n",
    "### c. Forward process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdJptif7PPGp"
   },
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQuV5AXnW2oq"
   },
   "source": [
    "With the **beta schedule** now defined, we can move on to the implementation of the **forward diffusion process**.\n",
    "\n",
    "Recall the following transition kernel which adds noise to $\\mathbf{x}_{t-1}$ to obtain $\\mathbf{x}_t$:\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta(t)} \\mathbf{x}_{t-1}, \\beta(t) \\mathit{\\boldsymbol{I}}).\n",
    "$$\n",
    "\n",
    "We can efficiently generate a noisy version of the input at any timestep $t$ **without iterating through all previous timesteps** using the following:\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathit{\\boldsymbol{I}}),\n",
    "$$\n",
    "\n",
    "where $\\bar{\\alpha}_t $ is the **cumulative product** of $(1 - \\beta_s)$ up to timestep $t$:\n",
    "  $$\n",
    "  \\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_s) = \\prod_{s=1}^{t} \\alpha_s.\n",
    "  $$\n",
    "\n",
    "Using the reparameterization trick, we can sample $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{\\epsilon}; \\mathbf{0}, \\mathit{\\boldsymbol{I}})$ and set  \n",
    "$$\n",
    "\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{(1 - \\bar{\\alpha}_t)} \\epsilon.\n",
    "$$\n",
    "\n",
    "Note the following:\n",
    "- The mean $ \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 $ ensures that the signal decreases gradually over time,\n",
    "- The variance term $(1 - \\bar{\\alpha}_t) \\mathit{\\boldsymbol{I}} $ ensures that the noise component increases over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzZfzP2XN_hB"
   },
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : You can use the following functions:\n",
    "- `torch.randn_like` : torch function to generate Gaussian noise of the same size of the input ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.randn_like.html))\n",
    "- `torch.cumprod`: torch function to do the cumulative product ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.cumprod.html))\n",
    "- `.view()`: method to reshape a tensor ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.view.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpjvvGE0W-PB"
   },
   "outputs": [],
   "source": [
    "# TODO implement the forward process\n",
    "\n",
    "\n",
    "def q_sample(\n",
    "    x_0: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    betas: torch.Tensor,\n",
    "    noise: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the forward process to sample a noisy version of x_0 at timestep t.\n",
    "\n",
    "    Args:\n",
    "        x_0 (torch.Tensor): The original clean data (image or feature vector).\n",
    "        t (torch.Tensor): Timestep tensor (batch of time indices).\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "        noise (torch.Tensor) or None : Optional tensor of noise to be added. If None, generates noise randomly.\n",
    "\n",
    "    Returns:\n",
    "        x_t (torch.Tensor): Noisy version of x_0 at timestep t.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0, device=x_0.device)\n",
    "\n",
    "    betas = betas.to(x_0.device)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = torch.sqrt(alphas_cumprod[t]).view(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - alphas_cumprod[t]).view(\n",
    "        -1, 1, 1, 1\n",
    "    )\n",
    "    # other correction using einops\n",
    "    # sqrt_alphas_cumprod_t = rearrange(torch.sqrt(alphas_cumprod[t]), \"b -> b 1 1 1\"\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GilNwb20Qrmv"
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08Qpbj7wog45"
   },
   "outputs": [],
   "source": [
    "# TODO: play along with the following visualization code to compare the different beta schedules.\n",
    "\n",
    "timesteps = 1001\n",
    "\n",
    "schedules = {\n",
    "    \"quadratic\": quadratic_beta_schedule,\n",
    "    \"linear\": linear_beta_schedule,\n",
    "    \"sigmoid\": sigmoid_beta_schedule,\n",
    "    \"cosine\": cosine_beta_schedule,\n",
    "}\n",
    "\n",
    "# Timesteps to visualize\n",
    "test_steps = np.arange(0, 1001, 100)\n",
    "\n",
    "\n",
    "# Load data sample\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: torch.rot90(x, dims=(1, 2))),\n",
    "        transforms.Lambda(\n",
    "            lambda x: 2 * (x - x.min()) / (x.max() - x.min()) - 1\n",
    "        ),  # Normalize the image between -1 and 1,\n",
    "        transforms.CenterCrop((52, 52)),\n",
    "    ]\n",
    ")\n",
    "index = np.random.randint(0, 10)\n",
    "x_0 = IXIDataset(img_dir, mode=\"train\", transform=transform)[index][\"T1\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(schedules), len(test_steps), figsize=(40, 15))\n",
    "\n",
    "for i, (name, schedule) in enumerate(schedules.items()):\n",
    "    betas = schedule(timesteps)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "    for j, t in enumerate(test_steps):\n",
    "        x_t = q_sample(x_0, t, betas)\n",
    "        axes[i][j].imshow(x_t.squeeze().numpy(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "        axes[i][j].set_xticks([])\n",
    "        axes[i][j].set_yticks([])\n",
    "\n",
    "        if i == 0:\n",
    "            axes[i][j].set_title(f\"t = {t}\")\n",
    "\n",
    "        if j == 0:\n",
    "            axes[i][j].set_ylabel(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O_zdK8xS6S2"
   },
   "outputs": [],
   "source": [
    "# TODO: play along with the following visualization code\n",
    "\n",
    "timesteps = 1001\n",
    "\n",
    "schedule = quadratic_beta_schedule\n",
    "\n",
    "# Load data sample\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: torch.rot90(x, dims=(1, 2))),\n",
    "        transforms.Lambda(\n",
    "            lambda x: 2 * (x - x.min()) / (x.max() - x.min()) - 1\n",
    "        ),  # Normalize the image between -1 and 1,\n",
    "        transforms.CenterCrop((52, 52)),\n",
    "    ]\n",
    ")\n",
    "index = np.random.randint(0, 10)\n",
    "x_0 = IXIDataset(img_dir, mode=\"train\", transform=transform)[index][\"T1\"]\n",
    "\n",
    "betas = schedule(timesteps)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for t in np.arange(0, timesteps, 5):\n",
    "    x_t = q_sample(x_0, t, betas)\n",
    "    imgs.append([plt.imshow(x_t.squeeze().numpy(), cmap=\"gray\", vmin=-1, vmax=1)])\n",
    "plt.close()\n",
    "ani = animation.ArtistAnimation(fig, imgs, interval=100, blit=True, repeat_delay=1000)\n",
    "\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WV0Vb2X5Zp0P"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_json": true,
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "clinicadl_beta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
