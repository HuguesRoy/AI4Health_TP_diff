{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qqmmsIDUOD0"
   },
   "source": [
    "# AI4Health Summer School - Pratical Session on Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiug6hc9k8bU"
   },
   "source": [
    "**Authors**: Hugues Roy, Maëlys Solal & Ninon Burgos\n",
    "\n",
    "For any comments or recommendation please contact: hugues.roy@inria.fr\n",
    "\n",
    "Many thanks to Charlotte Godard, Manon Heffernan & Swann Ruyter for their valuable feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U06C8v-mig98",
    "lines_to_next_cell": 0
   },
   "source": [
    "**Note**: If running in Colab, before starting, remember to change the runtime type to have access to GPU ressources: Runtime->Change Runtime Type, then choose GPU as hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Awjzl6oKZZfR"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U einops matplotlib tqdm torchtyping\n",
    "\n",
    "import math\n",
    "import os\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from IPython.display import HTML\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchtyping import TensorType\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from AI4Health_TP_diff.data import BraTSDataset\n",
    "from AI4Health_TP_diff.modules import (\n",
    "    Attention,\n",
    "    Block,\n",
    "    DiffusionModel,\n",
    "    Downsample,\n",
    "    LinearAttention,\n",
    "    PreNorm,\n",
    "    Residual,\n",
    "    ResnetBlock,\n",
    "    Upsample,\n",
    "    WeightStandardizedConv2d,\n",
    "    default,\n",
    "    exists,\n",
    ")\n",
    "from AI4Health_TP_diff.noise import rand_perlin_2d_octaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wra0YxD-WB1R"
   },
   "source": [
    "<a id=\"Introduction\"></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4a4gL-xig9-"
   },
   "source": [
    "In this lab, we will investigate diffusion models, and specifically, denoising diffusion probabilistic models (DDPM) as presented by [Ho et al, 2020](https://arxiv.org/abs/2006.11239).\n",
    "\n",
    "We will focus on **image synthesis**, in particular, the synthesis of **T1-weighted magnetic resonance (MR) images** from isotropic Gaussian noise, and apply these methods for anomaly detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Mathematical formulation of diffusion models](#maths)\n",
    "2. [Dataset manipulation](#data)\n",
    "3. [Crash course on PyTorch (optional)](#pytorch)\n",
    "4. [Implementation of all components necessary for training a diffusion model](#implementation): \n",
    "    - (a) [U-Net and time embeddings](#unet), \n",
    "    - (b) [Beta schedule](#beta), \n",
    "    - (c) [Forward process](#forward),\n",
    "    - (d) [Reverse process (from different perspectives)](#reverse)\n",
    "5. [Application to anomaly detection with the anoDDPM model](#anomdetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxA5qtJhY6kC"
   },
   "source": [
    "<a id=\"maths\"></a>\n",
    "## 1. Mathematical formulation of diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I88Opm1M8y3j"
   },
   "source": [
    "### a. Big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YLLwXOWl7J-"
   },
   "source": [
    "A diffusion model is a type of generative model, it models the data distribution in a way that allows to generate new samples.\n",
    "As VAEs and GANs, it converts noise from a simple distribution (typically isotropic Gaussian) to a data sample.\n",
    "In the most simple case, we train a neural network that learns to gradually denoise data from pure noise.\n",
    "\n",
    "It can be represented in the following way: <img src='https://drive.google.com/uc?id=11mjeMdwiTiPiAYgXsagEKAVW2iTILs7b'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in finding the parameter $\\theta^*$ of a model distribution $p_\\theta(\\mathbf{x})$ that best approximates the true, unknown data distribution $ p_{\\text{data}} $. To formalize this objective, we need a criterion that reflects how well $p_\\theta$ aligns with the observed data. One widely-used method is **maximum likelihood estimation**, which involves maximizing the expected log-likelihood of the data under $p_\\theta$.\n",
    "$$\n",
    "\\theta^* = \\mathrm{arg max} \\mathbb{E}_{\\mathbf{x} \\sim p_{\\text{data}}} \\left[ \\log p_{\\theta}(\\mathbf{x}) \\right]\n",
    "$$\n",
    "Here, the expectation is taken with respect to the empirical data distribution $p_{\\text{data}}$, representing the observed samples. By maximizing this quantity, we encourage the model $p_{\\theta}$​ to assign high probability to data drawn from the true distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJuZla7MOK96"
   },
   "source": [
    "It is a process consisting of two key components:\n",
    "\n",
    "1. **Fixed Forward Process $q$ (Noise Addition)**  \n",
    "   - Gradually adds noise to the input data according to a predefined **variance schedule**.\n",
    "   - Transforms the original data into pure noise over several timesteps.  \n",
    "\n",
    "2. **Learned Reverse Process $p_\\theta$ (Denoising)**  \n",
    "   - Aims to recover the original data from the noisy input.   \n",
    "   - This process is **learned by a neural network** (typically a U-Net), which predicts and removes the noise step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaAvVUE981T0"
   },
   "source": [
    "In practice, as for any generative model, we're interesting in maximising (log-)likelihood of the data distribution (which is intractable), so we maximise the evidence lower bound (ELBO).\n",
    "\n",
    "In the particular case of diffusion models, the ELBO can be written as:\n",
    "TODO insert equation 47 et 48.\n",
    "\n",
    "Note: parler vitef du fait que c'est une chaine de Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTTlGC9VmOTC"
   },
   "source": [
    "Both of these processes are indexed by time $t$, and are repeated a fixed number of times $T$.\n",
    "We start with $t=0$ and denote $\\mathbf{x}_0$ the real image from the data distribution.\n",
    "The forward process gradually samples noise that is added to the image at each timestep $t$.\n",
    "Given a sufficiently large $T$, and a good schedule for adding noise at each timestep, we should end up with an isotropic Gaussian distribution at $t=T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXZ2T9b2ORTi"
   },
   "source": [
    "### b. Forward process and beta schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCjnwAQtk7t3"
   },
   "source": [
    "Let $p_{\\text{data}}$ denote the real data distribution. We can sample from this distribution to obtain an image $\\mathbf{x}_0 \\sim p_{\\text{data}}$.\n",
    "\n",
    "The **forward diffusion process** progressively adds noise to this image at each timestep $t$. This noise is introduced according to a **beta schedule (or variance schedule)** $\\beta_t$, which determines the magnitude of noise added at each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWAWKhRJk2lI"
   },
   "source": [
    "#### Transition kernel: how to noise $\\mathbf{x}_{t-1}$ to obtain $\\mathbf{x}_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTMPUGiwlKuy"
   },
   "source": [
    "Mathematically, this can be written with the following transition kernel, which defines how to go from $\\mathbf{x}_{t-1}$ to $\\mathbf{x}_t$ (by adding noise):\n",
    "$$q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta(t)} \\mathbf{x}_{t-1}, \\beta(t) \\mathit{\\boldsymbol{I}}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtL24MuqlHcM"
   },
   "source": [
    "#### Reparameterization trick\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOGgkNDXOT2F"
   },
   "source": [
    "The reparameterization trick allows to obtain a sample from $\\mathcal{N}(\\mu, \\sigma^2)$ by sampling from $\\mathcal{N}(0, 1)$. Indeed, to obtain $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we can sample $\\epsilon \\sim \\mathcal{N}(0, 1)$ and set $x = \\mu + \\sigma \\cdot \\epsilon$.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1hnnqq2oCJPrGIvZtY4sMfRVdbYXunNV1'>\n",
    "\n",
    "\n",
    "\n",
    "In our case, we can sample $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{\\epsilon}; \\mathbf{0}, \\mathit{\\boldsymbol{I}})$ and set\n",
    "$$\n",
    "\\mathbf{x}_t = \\sqrt{1 - \\beta(t)} \\mathbf{x}_{t-1} + \\beta(t) \\epsilon.\n",
    "$$\n",
    "\n",
    "For instance, we can obtain $x_1$ from $x_0$,\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1RnZPkrmq801dl1KpcGv8geZNpzUoL2T-'>\n",
    "\n",
    "and then $x_2$ from $x_1$,\n",
    "<img src='https://drive.google.com/uc?id=1EXKJY9pux9Fvzj6n9m7PGEyxCNd7CXU3'>\n",
    "\n",
    "which we can rewrite as:\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1i1zyoehX6mK7Z6Hrlqg-PJ9izeQaxvGr'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu_o9nJ2lO44"
   },
   "source": [
    "#### Sampling directly a noisy version of the data: obtaining $\\mathbf{x}_{t}$ from $\\mathbf{x}_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFcBZsbLlCjz"
   },
   "source": [
    "In practice, rather than explicitely adding noise step by step over multiple timesteps, we can directly sample a noisy version of the data $\\mathbf{x}i_t$ from $\\mathbf{x}_0$ using a closed-form solution:\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathit{\\boldsymbol{I}}),\n",
    "$$\n",
    "\n",
    "where $\\bar{\\alpha}_t $ is the **cumulative product** of $(1 - \\beta_s)$ up to timestep $t $:\n",
    "$$\n",
    "\\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_s).\n",
    "$$\n",
    "\n",
    "We also use the reparameterization trick to sample $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{\\epsilon}; \\mathbf{0}, \\mathit{\\boldsymbol{I}})$ and set  \n",
    "$$\n",
    "\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + (1 - \\bar{\\alpha}_t) \\epsilon.\n",
    "$$\n",
    "\n",
    "Schematically, we have:\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1txwVhPqoo3xo2pH8GfS4uskBrAWJUswM'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ll-EHWrvy3Bs"
   },
   "source": [
    "\n",
    "### c. Reverse process\n",
    "\n",
    "**The goal**\n",
    "\n",
    "We would like to reverse the forward diffusion process, and to do that, we use a neural network to approximate the reverse transitions at each timestep $t$, i.e. we learn to denoise the data at each timestep.\n",
    "Ideally, we would like to approximate all the transitions $p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+1})$ for each timestep $t$, so we would like to train a neural network to model the distribution $p_{\\theta}(\\mathbf{x}_t \\mid \\mathbf{x}_{t+1})$.\n",
    "\n",
    "**The challenge**\n",
    "\n",
    "Unfortunately, we don't have a direct access to $p(\\mathbf{x}_t \\mid \\mathbf{x}_{t+1})$. During training, we only have access to: samples $\\mathbf{x}_0$ (from the training set), and the closed-form of the forward process (since it is fixed).\n",
    "\n",
    "This means that we are interested in learning the entire reverse chain, rather than only the transition from one step to another. In other words, we want consistency such as the output can be the input of the next step.\n",
    "\n",
    "\n",
    "**Objective: maximize the likelihood**\n",
    "\n",
    "\n",
    "We recall that as for any generative model, our objective is to maximize the log likelihood of the data distribution.\n",
    "In our case, we are trying to maximize the evidence lower bound (or ELBO), since attempting to maximize the log likelihood of the data distribution is intractable:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{x}_0) & \\geq \\mathbb{E}_{q(\\mathbf{x}_1, \\ldots, \\mathbf{x}_T|\\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_0,\\ldots,\\mathbf{x}_T)}{q(\\mathbf{x}_1,\\ldots,\\mathbf{x}_T | \\mathbf{x}_0)}\\right], \\quad \\text{(ELBO)}\\\\\n",
    "&= \\mathbb{E}_{q(\\mathbf{x}_1, \\ldots, \\mathbf{x}_T|\\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_T)\\prod_{t=1}^{T} p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{\\prod_{t=1}^{T} q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}\\right] \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "**Breaking down the loss**\n",
    "\n",
    "\n",
    "We can rewrite it as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} & = \\mathbb{E}_{q(\\mathbf{x}_{0}|\\mathbf{x}_1)} \\left[ \\log p_{\\theta} (\\mathbf{x}_{1}|\\mathbf{x}_0) \\right] \\quad (\\text{reconstruction term})\\\\\n",
    "& - \\sum_{t=1}^{T} \\mathbb{E}_{q(\\mathbf{x}_{t-1}, \\mathbf{x}_{t+1}|\\mathbf{x}_0)} \\left[ \\mathcal{D}_{KL}(q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1}) || p_{\\theta}(\\mathbf{x}_{t}|\\mathbf{x}_{t+1}))\\right] \\quad (\\text{consistency term}) \\\\\n",
    "& - \\mathbb{E}_{q(\\mathbf{x}_{t}|\\mathbf{x}_0)} \\left[ \\mathcal{D}_{KL}(q(\\mathbf{x}_{T}|\\mathbf{x}_{T-1}) || p_{\\theta}(\\mathbf{x}_{T}))\\right], \\quad (\\text{prior matching term}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "with $D_{KL}$ the Kullback Leibler divergence (it measures distance between distributions, and in our case, the distance between the model probability distribution and the true probability distribution).\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1RDq6nq7xMMcX4pJBl08aDQFMuiGcBHcE'>\n",
    "\n",
    "\n",
    "Looking at the loss $\\mathcal{L}$ above, we notice the following components:\n",
    "- a **reconstruction term**, which simply measures how well the final step of the reverse process reconstructs a data point on average,\n",
    "- a **prior matching term**, where we fix the prior distribution $p_{\\theta}(\\mathbf{x}_T)$ to be standard isotropic Gaussian, it is equal  $q(\\mathbf{x}_T \\mid \\mathbf{x}_{T-1})$, we do not need to optimize it (since it is fixed), \n",
    "- a consistency term, which ensures that for each intermediate latent variable $\\mathbf{x}_t$, the reverse model distribution $p_{\\theta}(\\mathbf{x}_t \\mid \\mathbf{x}_{t+1})$ matches the forward noising process $q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})$, thereby making the forward and reverses processes consistent.\n",
    "It is this last consistency term that we must optimize.\n",
    "\n",
    "In the figure, we can see that we should optimize the salmon distribution to match the blue one.\n",
    "\n",
    "\n",
    "**Problem**\n",
    "\n",
    "\n",
    "However, this is difficult to train because it requires computing an expectation over two random variables associated with the data point. In the figure, we display only a single trajectory, but  imagine the number of possible trajectories, it is enormous! This makes it challenging to directly match these distributions during training.\n",
    "\n",
    "\n",
    "**The trick**\n",
    "\n",
    "\n",
    "To address this issue, we use a small trick: since the forward process is a Markov chain, we can inject additional information without changing the forward dynamics. Specifically, during training, we condition on the original image $\\mathbf{x}_0$, which we have access to:\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)\n",
    "$$\n",
    "\n",
    "\n",
    "Why? Because in the ELBO we can rewrite:\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_1,\\ldots,\\mathbf{x}_T | \\mathbf{x}_0) = q(\\mathbf{x}_T|\\mathbf{x}_0) \\prod_{t=2}^{T} q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0} )\n",
    "$$\n",
    "\n",
    "\n",
    "So that if we recompute the ELBO using the above equation:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{x}_0) & \\geq \\mathbb{E}_{q(\\mathbf{x}_1, \\ldots, \\mathbf{x}_T|\\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_0,\\ldots,\\mathbf{x}_T)}{q(\\mathbf{x}_1,\\ldots,\\mathbf{x}_T | \\mathbf{x}_0)}\\right], \\quad \\text{(ELBO)}\\\\\n",
    "&= \\mathbb{E}_{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)} \\left[ \\log \\frac{p(\\mathbf{x}_T)\\prod_{t=1}^{T} p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_T|\\mathbf{x}_0) \\prod_{t=2}^{T} q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_{0} )}\\right] \\\\\n",
    "& = \\sum_{t=0}^{T} \\mathcal{L}_t\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "So we have:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_0 & = \\mathbb{E}_{q(\\mathbf{x}_{0}|\\mathbf{x}_1)} \\left[ \\log p_{\\theta} (\\mathbf{x}_{1}|\\mathbf{x}_0) \\right] \\quad (\\text{reconstruction term})\\\\\n",
    "\\mathcal{L}_{t-1} & = -\\mathbb{E}_{q(\\mathbf{x}_{t}|\\mathbf{x}_0)} \\left[ \\mathcal{D}_{KL}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0} ) || p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}))\\right] \\quad (\\text{consistency term}) \\\\\n",
    "\\mathcal{L}_{T} & = \\mathbb{E}_{q(\\mathbf{x}_{t}|\\mathbf{x}_0)} \\left[ \\mathcal{D}_{KL}(q(\\mathbf{x}_{T}|\\mathbf{x}_{0}) || p_{\\theta}(\\mathbf{x}_{T})), \\quad (\\text{pior matching term}) \\right] \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "This lets us reformulate the optimization: instead of approximating $q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1})$, we now aim to learn $p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t)$ to match the true posterior $q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0)$, which has a closed-form Gaussian expression that we will derive. The posterior describes how to denoise a noisy sample $\\mathbf{x}_t$ given access to the clean image $\\mathbf{x}_0$.\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1i0aJBnHAv5UWYsFNt1g-EguuyUTWKunU'>\n",
    "\n",
    "\n",
    "Now, the expectation only involves a single random variable! Great! Let's derive the closed-form expression of the posterior.\n",
    "\n",
    "\n",
    "**Closed-form of the posterior**\n",
    "\n",
    "\n",
    "Thanks to Bayes' rule, we can compute all the components required to derive this posterior. Bayes' rule gives:\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0) = \\frac{q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\, q(\\mathbf{x}_t \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}\n",
    "$$\n",
    "\n",
    "\n",
    "Rearranging terms, we obtain (because of the Markov chain: $q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}, \\mathbf{x}_0)$) :\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) = \\frac{q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) \\, q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_0)}{q(\\mathbf{x}_t \\mid \\mathbf{x}_0)}\n",
    "$$\n",
    "\n",
    "\n",
    "After simplifying all the terms, we find that:\n",
    "\n",
    "\n",
    "$$\n",
    "q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\propto \\mathcal{N}(\\boldsymbol{\\mu}_q, \\mathbf{\\Sigma}_q)\n",
    "$$\n",
    "\n",
    "\n",
    "with:\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_q = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) \\mathbf{x}_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t) \\mathbf{x}_0}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma}_q = \\frac{(1 - \\alpha_t)(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t} \\mathbf{I} = \\sigma_q \\mathbf{I}\n",
    "$$\n",
    "\n",
    "\n",
    "We showed that $q(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t, \\mathbf{x}_0) \\propto \\mathcal{N}(\\boldsymbol{\\mu}_q, \\mathbf{\\Sigma}_q)$, so the reverse process is also Gaussian. Let's simplify the loss!\n",
    "\n",
    "\n",
    "**Simplifying the loss**\n",
    "\n",
    "\n",
    "Because we have Gaussians, the KL divergences in the loss are simply the mean squared error between the two means weighted by the variance.\n",
    "\n",
    "\n",
    "$$\n",
    "D_{KL}(\\mathcal{N}(\\boldsymbol{\\mu}_q, \\mathbf{\\Sigma}_q) || \\mathcal{N}(\\boldsymbol{\\mu}_{\\theta}, \\mathbf{\\Sigma}_q)) = \\frac{1}{2 \\sigma_q} \\| \\boldsymbol{\\mu}_q - \\boldsymbol{\\mu}_{\\theta} \\|^{2}_{2}\n",
    "$$\n",
    "\n",
    "\n",
    "But if we look at $\\boldsymbol{\\mu}_q$\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_q = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) \\mathbf{x}_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t) \\mathbf{x}_0}{1 - \\bar{\\alpha}_t},\n",
    "$$\n",
    "\n",
    "\n",
    "we notice that everything except $\\mathbf{x}_0$ is known or given as input:\n",
    "- all $\\alpha_t$ coefficients are known (since they come from the variance schedule),\n",
    "- and $\\mathbf{x}_t$ is sampled using the forward $q( \\mathbf{x}_t | \\mathbf{x}_0)$.\n",
    "\n",
    "So we can rewrite the mean of the model distribution $p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_{t})$, and we see that the network is trying to predict the data point $\\mathbf{x}_0$, and we denote the prediction by $\\hat{\\mathbf{x}}_{\\theta}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_{\\theta} = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) \\mathbf{x}_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t) \\hat{\\mathbf{x}}_{\\theta}}{1 - \\bar{\\alpha}_t}\n",
    "$$\n",
    "\n",
    "\n",
    "**Data predictor loss**\n",
    "\n",
    "\n",
    "So substituting the loss, we finally have:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "L(\\theta) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_t)}{2 \\sigma_q (1 - \\bar{\\alpha}_t)} \\mathbb{E}_{t,\\mathbf{x}_0,\\mathbf{x}_t} \\left[ \\| \\mathbf{x_0} - \\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t,t) \\|^2 \\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "It involves the mean squared error between the true data point and the prediction during training.​​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvP9MlsOP5E_"
   },
   "source": [
    "### d. Reverse process and U-Net model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtsD_YOAmAxv"
   },
   "source": [
    "\n",
    "The neural network is designed to take a noisy image at a given timestep as input and output the predicted noise. The network can be used to predict either the data, denoted by $\\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t,t)$, the noise, denoted by $\\hat{\\boldsymbol\\epsilon}_{\\theta}(\\mathbf{x}_t,t)$, or the score, given by $\\mathbf{s}_{\\theta}(\\mathbf{x}_t,t) $. The UNet takes as input the **noisy image** $\\mathbf{x}_t$ and the corresponding **timestep** $t$ (contrary to the UNet for segmentation that just takes the image).\n",
    "Note that the predicted output is a tensor of the same dimensions as the input image $\\mathbf{x}_t$.  \n",
    "\n",
    "In this lab session, we will use a **conditional U-Net**. The primary distinction between this architecture and a conventional U-Net is that the upsampling and downsampling blocks incorporate an additional timestep parameter in their forward pass.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZp_J61QZAnz"
   },
   "source": [
    "<a id=\"data\"></a>\n",
    "## 2. Dataset manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFPNEVEfev96"
   },
   "source": [
    "We will be using a brain imaging dataset called [IXI](https://brain-development.org/ixi-dataset/). This dataset contains nearly 600 MR images from normal, healthy subjects, with different MR acquisitions. We will focus on T1-weighted and T2-weighted MR images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wPDNzzijJ4B"
   },
   "source": [
    "### a. Data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuZHziafm2lb"
   },
   "source": [
    "The dataset can be found on this [server](https://aramislab.paris.inria.fr/files/data/databases/DL4MI/IXI-dataset.tar.gz) and alternatively in the following [GitHub repository](https://github.com/Easternwen/IXI-dataset).\n",
    "In the `size64` folder, there are 1154 files: 2 images for 577 subjects. The size of each image is (64, 64), i.e. a slice of the 3D image acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "POvCf7gjig9_",
    "outputId": "b0ac118b-ab83-4a16-bd02-e25fd52b55db"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from GitHub\n",
    "! git clone https://github.com/Easternwen/IXI-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6S2PNSnjqTj"
   },
   "source": [
    "### b. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-EJlNSfig-A"
   },
   "source": [
    "There are two types of structural MR images: T1-weighted (T1w) images and T2-weighted (T2w) images.\n",
    "\n",
    "These imaging sequences do not highlight the same tissues: for example the cerebrospinal fluid (CSF)\n",
    "voxels are cancelled in T1w imaging whereas they are highlighted by\n",
    "the T2w imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "mTV0-ORtig-A",
    "outputId": "8eea1048-1349-4e62-9023-0dbde5dcdf50"
   },
   "outputs": [],
   "source": [
    "img_dir = \"./IXI-dataset/size64/\"\n",
    "\n",
    "sub_nb = \"013\"\n",
    "\n",
    "t1_path = os.path.join(img_dir, f\"sub-IXI{sub_nb} - T1.pt\")\n",
    "t2_path = os.path.join(img_dir, f\"sub-IXI{sub_nb} - T2.pt\")\n",
    "\n",
    "t1_img = torch.load(t1_path, weights_only=False)\n",
    "t2_img = torch.load(t2_path, weights_only=False)\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.swapaxes(t1_img, 0, 1), cmap=\"gray\", origin=\"lower\")\n",
    "plt.title(f\"T1 slice for subject {sub_nb}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.swapaxes(t2_img, 0, 1), cmap=\"gray\", origin=\"lower\")\n",
    "plt.title(f\"T2 slice for subject {sub_nb}\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQoAEJ5XrWIQ"
   },
   "source": [
    "### c. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUDgi3mnrX8R"
   },
   "source": [
    "Pre-processing of neuroimaging data (and medical imaging data in general) is essential before doing any experiment and especially before training a neural network.\n",
    "It allows standardizing and improving the quality of the data, to ensure that the deep neural network can learn meaninghul patterns and make accurate predictions.\n",
    "\n",
    "In this section, we go over common pre-processing steps for neuroimaging data for deep learning pipelines, and detail the pre-processing procedure for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-3xYWHwrbVU"
   },
   "source": [
    "#### Common pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7orAKygrcdC"
   },
   "source": [
    "**Registration**\n",
    "\n",
    "Registration consists of spatially aligning two or more images, either globally (rigid and affine registration) or locally (non-rigid registration), so that voxels in corresponding positions contain comparable information.\n",
    "It helps to standardize the neuroimaging data so that it is consistent across different subjects, scanners, and imaging modalities. This makes it easier for the deep neural network to learn patterns and make accurate predictions.\n",
    "\n",
    "**Bias field correction**\n",
    "\n",
    "MR images can be corrupted by a low frequency and smooth signal caused by magnetic field inhomogeneities. This bias field induces variations in the intensity of the same tissue in different locations of the image, which deteriorates the performance of image analysis algorithms such as registration.\n",
    "\n",
    "**Intensity normalization**\n",
    "\n",
    "Can help improve the performance of deep neural network.\n",
    "\n",
    "**Cropping**\n",
    "\n",
    "Some specific regions of the registered images can be selected in order to remove the background and to reduce the computing power required when training deep learning models.\n",
    "\n",
    "\n",
    "**Motion correction and noise reduction**\n",
    "\n",
    "Can help to minimize sources of noise and improve the quality of the data.\n",
    "Neuroimaging data can be noisy due to a variety of factors, such as head motion, scanner artifacts, and biological variability.\n",
    "\n",
    "**Feature extraction**\n",
    "\n",
    "Pre-processing can be used to extract features from the neuroimaging data that are relevant to the task at hand.\n",
    "For example, if the goal is to classify brain regions based on their functional connectivity, pre-processing may involve computing correlation matrices from the fMRI time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyGhLEmGrpqE"
   },
   "source": [
    "#### Our pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi7NYpx0rT_1"
   },
   "source": [
    "In our case the data has already been pre-processed with the following steps:\n",
    "- both the T1 and T2 images have undergone bias field correction using N4BiasFieldCorrection,\n",
    "- the T1 image was affinely registered to a template (MNI space),\n",
    "- the T2 image was affinely registered to the T1 image in template space.\n",
    "\n",
    "Finally, 30 central axial 2D slices were extracted from the 3D images.\n",
    "\n",
    "Below, you will find images for each of these steps for subjects 084 and 184.\n",
    "<img src='https://drive.google.com/uc?id=1vCs9gRqqCzpR7QzhNABgelM7mES0dRue'>\n",
    "<img src='https://drive.google.com/uc?id=1w_Bd4NNCxWStqVLNFEg022XFwtmVyzqQ'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub_h0FjKkLc7"
   },
   "source": [
    "### d. Dataset class and dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJ09GYzRig-B"
   },
   "source": [
    "Let's create a custom `IXIDataset` class to easily access the data.\n",
    "\n",
    "We split the dataset between training and testings sets. The training set contains 80% of the images, and the testing set contains the remaining 20%.\n",
    "We will use the `train` or `test` mode of the `IXIDataset` class to access training or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZTzhiTgig-C",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class IXIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset utility class.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Path of the folder with all the images.\n",
    "        mode (str) {'train' or 'test'}:  Part of the dataset that is loaded.\n",
    "        transform (callable):  Optional transform to be applied on a sample.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, mode=\"train\", transform=None):\n",
    "        files = sorted(os.listdir(img_dir))\n",
    "        patient_id = list(set([i.split()[0] for i in files]))\n",
    "\n",
    "        imgs = []\n",
    "\n",
    "        if mode == \"train\":\n",
    "            for i in patient_id[: int(0.8 * len(patient_id))]:\n",
    "                t1_path = os.path.join(img_dir, i + \" - T1.pt\")\n",
    "                t2_path = os.path.join(img_dir, i + \" - T2.pt\")\n",
    "                if os.path.isfile(t1_path) and os.path.isfile(t2_path):\n",
    "                    imgs.append((t1_path, t2_path))\n",
    "\n",
    "        elif mode == \"test\":\n",
    "            for i in patient_id[int(0.8 * len(patient_id)) :]:\n",
    "                t1_path = os.path.join(img_dir, i + \" - T1.pt\")\n",
    "                t2_path = os.path.join(img_dir, i + \" - T2.pt\")\n",
    "                if os.path.isfile(t1_path) and os.path.isfile(t2_path):\n",
    "                    imgs.append((t1_path, t2_path))\n",
    "\n",
    "        self.imgs = imgs\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t1_path, t2_path = self.imgs[index]\n",
    "\n",
    "        t1 = torch.load(t1_path, weights_only=False)[None, :, :]\n",
    "        t2 = torch.load(t2_path, weights_only=False)[None, :, :]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            t1 = transform(t1)\n",
    "            t2 = transform(t2)\n",
    "\n",
    "        return {\"T1\": t1, \"T2\": t2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCcmPANyig-C"
   },
   "source": [
    "Using this class and the `DataLoader` class from `torch.utils.data`, we can\n",
    "easily access our dataset. Here is a quick example on how to use it:\n",
    "\n",
    "```python\n",
    "# Create a DataLoader instance for the training set\n",
    "dataloader = DataLoader(\n",
    "    IXIDataset(img_dir, mode=\"train\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# You will get a batch of samples from the training set\n",
    "for batch in dataloader:\n",
    "    # batch is a dictionary with two keys:\n",
    "    # - batch[\"T1\"] is a tensor with shape (batch_size, 64, 64) with the T1 images for the samples in this batch\n",
    "    # - batch[\"T2\"] is a tensor with shape (batch_size, 64, 64) with the T2 images for the samples in this batch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV0Vb2X5Zp0P"
   },
   "source": [
    "<a id=\"pytorch\"></a>\n",
    "## 3. Crash course on neural network layers with PyTorch (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19MddTXkst0O"
   },
   "source": [
    "In a neural network everything is called a layer although the operations performed in the layers may be very different. You will find below a summary of some of the different operations that may be performed in a neural network (and more specifically, a convolutional neural network).\n",
    "\n",
    "As medical images are often 3D, we introduce the operations performed by the neural network layers in the 3D case.\n",
    "In the implementation, we will use their 2D counterparts since our images are 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lklFFbIKA5WS"
   },
   "source": [
    "### Multi-Layer Perceptron\n",
    "A multi-layer perceptron (MLP) is a simple type of feedforward artificial neural network that consists of multiple layers of neurons connected in a fully connected (or dense) manner.\n",
    "\n",
    "It allows to approximate any continuous function $f : \\mathbb{R}^m \\to \\mathbb{R}^o$, to map a set of features $X = (x_1, x_2, \\ldots, x_m) \\in \\mathbb{R}^m$ to a label $y \\in \\mathbb{R}^o$.\n",
    "\n",
    "Its key components include:\n",
    "- the input layer: receives the raw input features, and does not perform any computation,\n",
    "- the hidden layers: one or more layers of neurons, where each neuron computes a weighted sum of its inputs, adds a bias and applies a nonlinear activation function,\n",
    "- the output layer which produces the final layer, often by using a softmax activation for classification or linear activation for regression.\n",
    "\n",
    "Here is a schematic diagram:\n",
    "<img src='https://drive.google.com/uc?id=1kDmvTPFBnG2_X04bPbSHXnmda5eXJFGW'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt_5UpLIiTzM"
   },
   "source": [
    "### Feature maps\n",
    "\n",
    "The outputs of the layers in a convolutional neural network are called feature maps. In the case of 3D images, their size is written with the format `n_channels @ dim1 x dim2 x dim3`, and in the case of 2D images (our case) `n_channels @ dim1 x dim2`.\n",
    "\n",
    "For a 2D CNN the dimension of the feature maps is actually 4D as the first dimension is the batch size. This dimension is added by the `DataLoader` of pytorch which stacks the 3D tensors computed by a `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1VVZU27iXCR",
    "outputId": "72672648-6688-4502-d6ca-7482c12fdd73"
   },
   "outputs": [],
   "source": [
    "img_dir = \"./IXI-dataset/size64/\"\n",
    "batch_size = 1\n",
    "\n",
    "dataset = IXIDataset(img_dir, mode=\"train\")\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "data = next(iter(dataloader))\n",
    "\n",
    "print(\"Shape of IXIDataset output:\", dataset[0][\"T1\"].shape)\n",
    "print(\"Shape of DataLoader output:\", data[\"T1\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK_IkxwliGve"
   },
   "source": [
    "\n",
    "### 3D convolutions (`nn.Conv3d`)\n",
    "\n",
    "[Link to the PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html)\n",
    "\n",
    "The main arguments of this layer are the input channels (`in_channels`), the output channels (number of filters trained) (`out_channels`) and the size of the filter (or kernel) (`kernel_size`).\n",
    "\n",
    "For the `kernel_size` parameter, if an integer `k` is given the kernel will be a cube of size `k * k * k`. It is possible to construct rectangular kernels by entering a tuple (but this is very rare).\n",
    "\n",
    "You will find below an illustration of how a single filter produces its output feature map by parsing one feature map. The size of the output feature map produced depends of the convolution parameters and can be computed with the following formula:\n",
    "\n",
    "$$O_i = \\frac{I_i-k+2P}{S} + 1$$\n",
    "\n",
    "*   $O_i$ the size of the output along the ith dimension\n",
    "*   $I_i$ the size of the input along the ith dimension\n",
    "*   $k$ the size of the kernel\n",
    "*   $P$ the padding value\n",
    "*   $S$ the stride value\n",
    "\n",
    "In the following example $\\frac{5-3+2*0}{1}+1 = 3$\n",
    "\n",
    "![2D convolutional layer gif](https://drive.google.com/uc?id=166EuqiwIZkKPMOlVzA-v5WemJE2tDCES)\n",
    "\n",
    "To be able to parse all the feature maps of the input, a filter is actually a 4D tensor of size `(input_channels, k, k, k)`. The set of all filters included in a convolutional layer is then a 5D tensor stacking all the filters of size `(output_channels, input_channels, k, k, k)`.\n",
    "\n",
    "Each filter is also associated to a bias value that is a scalar added to all the feature maps it produces. Then the bias is a 1D vector of size `output_channels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETjl7kp17-IM",
    "outputId": "54cc6fe3-e9f3-4652-c9ab-b1c1e099314a"
   },
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv3d(8, 16, 3)\n",
    "print(\"Weights shape:\", conv_layer.weight.shape)\n",
    "print(\"Bias shape:\", conv_layer.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0ItmAuXBM9P"
   },
   "source": [
    "### Weight Standardized Convolution (WeightedStandardizedConv2d, WSConv)\n",
    "\n",
    "Weight Standardized Convolution is a technique introduced in [(Qiao et al., 2019)](https://arxiv.org/pdf/1903.10520) to improve the training stability of convolutional neural networks, especially when batch sizes are small.\n",
    "\n",
    "The core idea is to normalize the kernel weights of the convolution filters (before performing the convolution operation), rather than the activations.\n",
    "It leads to more stable gradients, especially in small-batch settings, which llows stabilizing training.\n",
    "It also works well with GroupNorm (that we evoke later in the tutorial), and prevents weight explosion or collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMmdLHtSA9tZ"
   },
   "source": [
    "### Batch Normalization (`nn.BatchNorm3d`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.BatchNorm3d.html#torch.nn.BatchNorm3d)\n",
    "\n",
    "Learns to normalize feature maps according to [(Ioffe & Szegedy, 2015)](https://arxiv.org/abs/1502.03167). The following formula is applied on each feature map  $FM_i$:\n",
    "\n",
    "$$FM^{normalized}_i = \\frac{FM_i - mean(FM_i)}{\\sqrt{var(FM_i) + \\epsilon}} * \\gamma_i + \\beta_i$$\n",
    "\n",
    "*   $\\epsilon$ is a hyperparameter of the layer (default=1e-05)\n",
    "*   $\\gamma_i$ is the value of the scale for the ith channel (learnable parameter)\n",
    "*   $\\beta_i$ is the value of the shift for the ith channel (learnable parameter)\n",
    "\n",
    "This layer does not have the same behaviour during training and evaluation, this is why it is needed to put the model in evaluation mode in the test function with the command `.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QPASZehDHjc",
    "outputId": "daa2d4f7-9cb7-4c04-a14a-dda87a6ce036"
   },
   "outputs": [],
   "source": [
    "batch_layer = nn.BatchNorm3d(16)\n",
    "print(\"Gamma value:\", batch_layer.state_dict()[\"weight\"].shape)\n",
    "print(\"Beta value:\", batch_layer.state_dict()[\"bias\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXW9iQkeQsRz"
   },
   "source": [
    "### Group Normalization (nn.GroupNorm)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html)\n",
    "\n",
    "Applies group normalization over a mini-batch of inputs, as described in the paper [(Wu & He, 2018)](https://arxiv.org/abs/1803.08494).\n",
    "The following formula is applied:\n",
    "$$ y = \\frac{x - \\mathbb{E}(x)}{\\sqrt{\\mathbb{V}(x)+\\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "The layer takes as parameters:\n",
    "* `num_groups (int)` the number of groups to separate the channels into\n",
    "* `num_channels (int)` the number of channels expected in input\n",
    "* `eps (float)` a hyperparameter for numerical stability (default=1e-05)\n",
    "* `affine (bool)` a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases), denoted \\gamma and \\beta in the above formula\n",
    "\n",
    "The channels (or feature maps) are divided into smaller groups and the features within each group are normalized.\n",
    "\n",
    "Unlike with BatchNorm where normalization is done across examples in a batch, in GroupNorm, the normalization is done separately for each example in a batch, meaning that the normalization is independent of the batch size.\n",
    "This is particularly useful when working with small or variable batch sizes, which is common in diffusion models.\n",
    "Here is a schematic drawing of different types of normalizations:\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1FzBtm94yF5_uz4yV9TNla1rwuFGr1yeP'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4TxeR3lEZeI"
   },
   "source": [
    "### Activation function (`nn.LeakyReLU`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)\n",
    "\n",
    "In order to introduce non-linearity in the model, an activation function is introduced after the convolutions. It is applied on all intensities independently.\n",
    "\n",
    "The graph of the Leaky ReLU is displayed below, $\\alpha$ being a hyperparameter of the layer (`default=0.01`):\n",
    "\n",
    "![Leaky ReLU graph](https://sefiks.com/wp-content/uploads/2018/02/prelu.jpg?w=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dRn0vyuBXLc"
   },
   "source": [
    "### Another activation function (nn.SiLU)\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html)\n",
    "\n",
    "Another type of non-linear activation function is the SiLU function, or Sigmoid Linear Unit, also known as the swish function:\n",
    "$$ \\text{silu}(x) = x * \\sigma(x)$$\n",
    "where $\\sigma(x) : x \\mapsto \\frac{1}{1 + e^{-x}}$ is the logistic sigmoid.\n",
    "\n",
    "Compared to the Leaky ReLU, the SiLU is smooth and differentable everywhere, allowing for better optimization during training, especialy in deeper networks.\n",
    "Its monotonicity allows capturing more complex relationships.\n",
    "However, it is more expensive to compute.\n",
    "\n",
    "The graph of the SiLU is displayed below:\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1b4U2peBgMTOgLGu6idw0ruh2qbJAZd1b'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3Oflm0QGHHG"
   },
   "source": [
    "### Pooling function (`nn.MaxPool2d`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool3d.html)\n",
    "\n",
    "The structure of the pooling layer is very similar to the convolutional layer: a kernel is passed through the input with a defined size and stride. However there is no learnable parameters in this layer, the kernel outputs the maximum value of the part of the feature map it covers.\n",
    "\n",
    "Here is an example in 2D of the standard layer of pytorch `nn.MaxPool2d`:\n",
    "\n",
    "![nn.MaxPool2d behaviour](https://drive.google.com/uc?id=1qh9M9r9mfpZeSD1VjOGQAl8zWqBLmcKz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4Py8zLGX2vk"
   },
   "source": [
    "### Flatten (`nn.Flatten`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n",
    "\n",
    "This layer flattens the array to a 1D array. This operation is necessary between the 3D (or 2D) convolutions and the fully-connected layers.\n",
    "\n",
    "You also can perform the flatten operation in the `forward` method of the network with `view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9ctrE3_X4jH",
    "outputId": "d436fa75-699a-446f-91a1-5af847a2f97d"
   },
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "input_tensor = torch.rand(8, 16, 4, 5, 4)\n",
    "output_tensor = flatten(input_tensor)\n",
    "\n",
    "print(\"Shape of the output tensor:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-65jRniEjr2"
   },
   "source": [
    "### Dropout (`nn.Dropout`)\n",
    "\n",
    "[Link to PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "\n",
    "The aim of a dropout layer is to replace a fixed proportion of the input values by 0 during training only. This has proven to be an effective technique for regularization.\n",
    "\n",
    "This layer does not have the same behaviour during training and evaluation, this is why it is needed to put the model in evaluation mode in the test function with the command `.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0O3cCR7HGge",
    "outputId": "a326d142-0fb1-4e54-daff-a49ae7256551"
   },
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(0.5)\n",
    "input_tensor = torch.rand(10)\n",
    "output_tensor = dropout(input_tensor)\n",
    "print(\"Input:\", input_tensor)\n",
    "print(\"Output:\", output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hL2TdAnE-jn"
   },
   "source": [
    "### Fully-Connected Layers (`nn.Linear`)\n",
    "\n",
    "[PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "\n",
    "The fully connected (FC) layers take as input 2D vectors of size `(batch_size, N)`. They have two mandatory arguments, the number of values per batch of the input and the number of values per batch of the output.\n",
    "\n",
    "Each output neuron in a FC layer is a linear combination of the inputs + a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "um2OupjmGy94",
    "outputId": "42df5073-588e-473d-f339-f358d52ff6a7"
   },
   "outputs": [],
   "source": [
    "fc = nn.Linear(16, 2)\n",
    "print(\"Weights shape:\", fc.weight.shape)\n",
    "print(\"Bias shape:\", fc.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuRPuTmkig-C"
   },
   "source": [
    "<a id=\"implementation\"></a>\n",
    "## 4. Implementation of the diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEceqPi4bLCw"
   },
   "source": [
    "<a id=\"unet\"></a>\n",
    "### a. U-Net conditioning & time embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcHnjAyHSFze"
   },
   "source": [
    "#### Time Positional Embedding\n",
    "\n",
    "In this section, we explore how to represent timesteps in a neural network using **positional embeddings**.  \n",
    "\n",
    "**Why not use raw time values directly?**  \n",
    "\n",
    "Suppose we want to encode $ T = 500 $ timesteps. A straightforward approach might be to pass the raw timestep value $t$ directly into the network. However, this approach has several drawbacks:  \n",
    "\n",
    "- **Numerical instability**: Large values of  $t$ can introduce numerical instability, making it harder for the model to learn meaningful patterns.  \n",
    "- **Inconsistent scaling**: Normalizing time values (e.g., to a range between 0 and 1) can be problematic. Different sequences of varying lengths would be normalized differently, making it difficult for the model to generalize across sequences of different durations.  \n",
    "\n",
    "To address these issues, we use **time positional encoding**, which provides a unique representation for each timestep.  \n",
    "\n",
    "\n",
    "**Sinusoidal Positional Embedding**  \n",
    "\n",
    "Positional encoding assigns a distinct vector representation (embedding) to each timestep $t \\in [0,T]$, ensuring that no two timesteps share the same embedding. The classical **sinusoidal position embedding** is defined as follows:  \n",
    "\n",
    "$$\n",
    "P(t, 2i) = \\sin \\left( \\frac{t}{n^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(t, 2i+1) = \\cos \\left( \\frac{t}{n^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $0 \\leq i < d/2 $ maps the column indices,  \n",
    "- $d$ is the embedding dimension,  \n",
    "- $n$ is a user-defined scalar.  \n",
    "\n",
    "**Key Observations**  \n",
    "\n",
    "- The **sine function** is applied for the even-indexed columns, whereas the **cosine function** is applied for the odd-indexed columns.  \n",
    "- This method ensures that each timestep is uniquely represented in the range $[-1,1]$.  \n",
    "- The sinusoidal positional encoding introduces periodic patterns, which help the model capture **temporal dependencies** effectively. It maintains relative distance between positions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJWC8mBULxf2"
   },
   "source": [
    "[**TO DO**]: complete the sinusoidal embedding,\n",
    "\n",
    "As it is going to be passed trough a MLP, we can concatenate them directly. So we have:\n",
    "\n",
    "$$\n",
    "P(t, i) = \\sin \\left( \\frac{t}{n^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(t, \\frac{d}{2} + i) = \\cos \\left( \\frac{t}{n^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $0 \\leq i < d/2 $ maps the column indices,  \n",
    "- $d$ is the embedding dimension,  \n",
    "- $n$ is a user-defined scalar.\n",
    "\n",
    "\n",
    "\n",
    "We will use the following mathematical trick to improve numerical stability and efficiency:\n",
    "\n",
    "$$\n",
    "n^{\\frac{2i}{d}} = \\exp \\left( \\frac{2i}{d} \\log(n)\\right)\n",
    "$$\n",
    "\n",
    "and set $n = 10000 $ (it is related to the wavelength of the sinusoidal functions, the distance between the peaks of the sinusoidal functions increases exponentially as the position in the sequence increases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : we can use the function:\n",
    "- `torch.arange`: similar to numpy arange function ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.arange.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyNyCy7vxI57"
   },
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time: torch.Tensor):\n",
    "        # naive implementation\n",
    "\n",
    "        # TODO\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "\n",
    "        # HERE for the representation we want\n",
    "        # first half columns to be the sinus function #second half colmuns to be the cosine function\n",
    "\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtoJu0pFgshN"
   },
   "source": [
    "Here we plot an example for the timestep 10. We can see the associated embbeding (vector representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "DJc5zQBIWeDg",
    "outputId": "44e7f425-2671-4e71-b8a1-17fa38b05554"
   },
   "outputs": [],
   "source": [
    "sinusoidal_embedding = SinusoidalPositionEmbeddings(dim=64)\n",
    "n = 50\n",
    "times = torch.tensor([10])\n",
    "\n",
    "times_embedded = sinusoidal_embedding(times)\n",
    "\n",
    "plt.imshow(times_embedded.detach().numpy(), vmin=-1.0, vmax=1.0, interpolation=\"none\")\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.ylabel(\"time\")\n",
    "plt.yticks([0], [10], rotation=\"vertical\")\n",
    "plt.xlabel(\"embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMZwOw2IhBNR"
   },
   "source": [
    "Here we plot the embeddings for different times steps, and obtain a matrix. We can see the first half of the matrix corresponds to the sine function and the second half to the cosine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "vJRR5h_CSW_M",
    "outputId": "c5f336c8-bbf6-40a1-9ce9-58d17fa5e2b6"
   },
   "outputs": [],
   "source": [
    "n = 500\n",
    "timesteps = torch.arange(0, n)\n",
    "\n",
    "sinusoidal_embedding = SinusoidalPositionEmbeddings(dim=64)\n",
    "times_embedded = sinusoidal_embedding(timesteps)\n",
    "\n",
    "plt.imshow(\n",
    "    times_embedded.detach().numpy(),\n",
    "    vmin=-1.0,\n",
    "    vmax=1.0,\n",
    "    interpolation=\"none\",\n",
    "    aspect=0.1,\n",
    ")\n",
    "plt.xlabel(\"embedding\")\n",
    "plt.ylabel(\"time\")\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Vu0dI46hUnn"
   },
   "source": [
    "#### U-Net conditioning  \n",
    "\n",
    "Previously, we defined the **time positional embedding**, which maps a given timestep to a vector representation. However, how exactly should this embedding be incorporated into the U-Net?\n",
    "\n",
    "Remember that we would like a **reparameterization** of the network for each timestep.\n",
    "\n",
    "**Why Not Use Time Embedding Only at the Input Layer?**  \n",
    "\n",
    "One possible approach is to provide the time embedding **only at the beginning of the network**. However, this method has significant drawbacks:  \n",
    "\n",
    "- In **deep architectures**, the influence of the time embedding may **diminish** as information propagates through the network.  \n",
    "- The network may struggle to **retain** temporal information, reducing its ability to adapt effectively to different timesteps.  \n",
    "\n",
    "**A Better Approach: Conditioning Each U-Net Block**  \n",
    "\n",
    "To ensure that the time embedding remains influential throughout the network, we **condition each block** of the U-Net with the time embedding. This technique effectively **reparameterizes** the network for each timestep, ensuring that temporal information is **consistently and strongly integrated** at every stage.  \n",
    "\n",
    "By applying this conditioning mechanism:  \n",
    "- Each U-Net block can **adapt its behavior** based on the current timestep.  \n",
    "- The model can learn more expressive **time-dependent features**, improving its ability to process noisy images at different timesteps.  \n",
    "\n",
    "![network_schema_page-0001.jpg](data:image/jpeg;base64,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)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Scope of This Section**  \n",
    "\n",
    "In this section, we will not dive deeply into the **U-Net architecture itself**. Instead, we will focus specifically on how the **time embedding is incorporated** into the network.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7limK5a4-jNh"
   },
   "source": [
    "We will use the following class called Block to pass the information of the UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njyheuhcgML0"
   },
   "source": [
    "In diffusion models, a ResNet block is typically composed of multiple layers to apply time conditioning via **scale-shift normalization** (also called FiLM: feature-wise Linear Modulation ([link to paper](https://arxiv.org/abs/1709.07871))).\n",
    "\n",
    "The following figure shows the architecture:\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1W48p9bkS5biMBcCWnOJ6wZmp66O6mWdl' width='700'>\n",
    "\n",
    "and below is a detailed description of each layer:\n",
    "\n",
    "1. **Initial convolution**\n",
    "\n",
    "  A standardized convolution is applied to the input to project it into the desired feature space:\n",
    "\n",
    "  $$\n",
    "  x_1 = \\mathrm{Conv}(x_0)\n",
    "  $$\n",
    "\n",
    "2.  **Normalization**\n",
    "\n",
    "  The input tensor $x$ is normalized using a method such as GroupNorm:\n",
    "\n",
    "  $$\n",
    "  x_2 = \\mathrm{Norm}(x_1)\n",
    "  $$\n",
    "\n",
    "3. **Scale-Shift Modulation**\n",
    "\n",
    "   A time embedding $t$ is passed through a small MLP $\\Phi$ to produce scaling and shifting parameters $\\gamma$ and $\\beta$, applied channel-wise:\n",
    "\n",
    "   $$\n",
    "   (\\gamma, \\beta) = \\Phi(t)\n",
    "   $$\n",
    "\n",
    "   These parameters are applied channel-wise to the normalized features:\n",
    "\n",
    "   $$\n",
    "   x_3 = (1 + \\gamma) \\cdot x_2 + \\beta\n",
    "   $$\n",
    "\n",
    "   It is important that the normalization is done prior to the scale-shift modulation, so that the scale-shift modulation has an effect.\n",
    "\n",
    "4. **Activation**\n",
    "\n",
    "   A non-linear activation function (e.g., SiLU or GELU) is applied:\n",
    "\n",
    "   $$\n",
    "   x_4 = \\mathrm{Activation}(x_3)\n",
    "   $$\n",
    "\n",
    "   It comes after the conditioning, to ensure that the activation accounts for the timestep.\n",
    "\n",
    "5. **Convolution + Normalization + Activation**\n",
    "\n",
    "   A convolutional layer is applied to propagate local spatial information:\n",
    "\n",
    "   $$\n",
    "   x_7 = \\mathrm{Activation}(\\mathrm{Norm}(\\mathrm{Conv}(x_4)))\n",
    "   $$\n",
    "\n",
    "6. **Residual Connection**\n",
    "\n",
    "   The original input is added back to the transformed output. If the number of channels has changed, a linear projection $h(x_0)$ (typically a $1 \\times 1$ convolution) is used to align dimensions:\n",
    "\n",
    "   $$\n",
    "   \\text{output} = h(x_0) + x_7\n",
    "   \\quad \\text{or} \\quad\n",
    "   \\text{output} = x_0 + x_7\n",
    "   $$\n",
    "\n",
    "The residual connection is important because it helps preserve the structure of the data from one step to the next.\n",
    "Moreover, it allows the network to condition its internal computations on time.\n",
    "Specifically, it combines:\n",
    "\n",
    "- the original input (processed without time conditioning), and\n",
    "- the transformed features (modulated by the time embedding).\n",
    "\n",
    "This combination allows the block to add time-dependent refinements without discarding the essential spatial or semantic information in the input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : we can use the function:\n",
    "- `nn.GroupNorm`: torch module that applies group normalization ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html))\n",
    "- `nn.SiLU`: torch module that applies the SiLU activation function ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.SiLU.html))\n",
    "- `WeightStandardizedConv2d` : our custom module that applies weight standardization to a convolutional layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De5NHQdG-WQF"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, dim: int, dim_out: int, groups: int = 8):\n",
    "        super().__init__()\n",
    "        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        scale_shift: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if scale_shift:\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : you can use the block we defined before and:\n",
    "- `.chunk()` method : Attempts to split a tensor into the specified number of chunks. Each chunk is a view of the input tensor ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk))\n",
    "- `nn.Conv2D`: torch module to apply 2D Convolution ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, dim_out: int, time_emb_dim: int, groups: int = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time_emb: Optional[torch.Tensor] = None):\n",
    "        # TODO: implement the forward pass of the ResnetBlock\n",
    "\n",
    "        scale_shift = None\n",
    "\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            # TODO\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")\n",
    "            # time_emb = time_emb.view(time_emb.shape[0], time_emb.shape[1], 1, 1)\n",
    "\n",
    "            scale_shift = time_emb.chunk(2, dim=1)\n",
    "\n",
    "        h = self.block1(x, scale_shift=scale_shift)\n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unet**: The Unet architecture is <span style=\"color:red\">given</span>, it incorporates the resnet structure you defined before.\n",
    "\n",
    "\n",
    "This model is a hierarchical encoder-decoder (UNet) designed for diffusion models. It uses skip connections, attention, and time conditioning via sinusoidal embeddings. Here's a breakdown of each key component:\n",
    "\n",
    "1. **Stem**: A 1×1 convolution expands the number of channels in the input image. We also keep a copy of this high-resolution feature map to use later for the final skip connection.\n",
    "\n",
    "2. **Time embedding**: We use sinusoidal positional embeddings to encode timestep information. These embeddings are passed through an MLP, producing a time vector of shape (B, time_dim). This vector modulates all residual blocks via FiLM (Feature-wise Linear Modulation), injecting temporal information.\n",
    "\n",
    "3. **Encoder**: The encoder reduces spatial resolution while increasing the number of channels. At each level we have: two ResNet block with time conditioning, a linear Attention layer and a downsampling operation. Intermediate activations are saved in a stack h for later use in the decoder.\n",
    "\n",
    "4. **Bottleneck**: At the lowest spatial resolution: One ResNet block -> Attention -> ResNet block. This is the most compressed representation of the input.\n",
    "\n",
    "5. **Decoder**: The decoder progressively restores the spatial resolution. At each level: two skip connections from encoder are concatenated, two time-conditioned ResNet blocks, one linear attention layer and an upsampling operation (except at the top level)\n",
    "\n",
    "6. **Output layer**: The final decoder feature map is merged with the high-res skip from the stem. One last ResNet block refines the output. A 1x1 convolution to project to the desired number of output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTzMRqSPh81l"
   },
   "outputs": [],
   "source": [
    "class Unet(DiffusionModel):\n",
    "    \"\"\"\n",
    "    Hierarchical encoder-decoder with skip connections + attention,\n",
    "    conditioned on a sinusoidal time embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        init_dim: Optional[int] = None,\n",
    "        out_dim: Optional[int] = None,\n",
    "        dim_mults: Optional[Tuple[int, int, int]] = (1, 1, 2),\n",
    "        channels: Optional[int] = 1,\n",
    "        resnet_block_groups: Optional[int] = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "        self.channels = channels\n",
    "        input_channels = channels\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(\n",
    "            input_channels, init_dim, 1, padding=0\n",
    "        )  # changed to 1 and 0 from 7,3\n",
    "\n",
    "        # ---------- resolution schedule -------------------------------------------\n",
    "        # dims = [64, 64, 64, 128]  for dim=64, dim_mults=(1,1,2)\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # ResNet block template with fixed GroupNorm groups\n",
    "        block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "\n",
    "        # ---------- time embedding MLP --------------------------------------------\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "\n",
    "        # ---------- encoder (downs) layers ------------------------------------------------\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Downsample(dim_in, dim_out)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ---------- bottleneck -----------------------------------------------------\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "\n",
    "        # ---------- decoder (ups) --------------------------------------------------\n",
    "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Upsample(dim_out, dim_in)\n",
    "                        if not is_last\n",
    "                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ---------- decoder (ups) --------------------------------------------------\n",
    "        self.out_dim = default(out_dim, channels)\n",
    "        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)\n",
    "        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, time: torch.Tensor):\n",
    "        x = self.init_conv(x)\n",
    "\n",
    "        r = x.clone()\n",
    "\n",
    "        # -------- time conditioning ----------------------------------------------\n",
    "        t = self.time_mlp(time)\n",
    "        h = []\n",
    "\n",
    "        # -------- encoder ---------------------------------------------------------\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        # -------- bottleneck ------------------------------------------------------\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        # -------- decoder ---------------------------------------------------------\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        # -------- final head ------------------------------------------------------\n",
    "        x = torch.cat((x, r), dim=1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXy0a8OqIY8C"
   },
   "source": [
    "<a id=\"beta\"></a>\n",
    "### b. Beta schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAjdtx3hWxp4"
   },
   "source": [
    "\n",
    "The forward diffusion process progressively adds noise to an image. This noise is introduced according to a known **variance schedule (or beta schedule)** $\\beta_t$, which determines the magnitude of noise added at each timestep $t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSXWFymCKNcV"
   },
   "source": [
    "#### Types of beta schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu6TLD6QKYzi"
   },
   "source": [
    "The choice of the beta schedule significantly impacts the model's performance. Common schedules include:  \n",
    "- **Linear beta schedule**  \n",
    "- **Cosine beta schedule**  \n",
    "- **Quadratic beta schedule**  \n",
    "- **Sigmoid beta schedule**  \n",
    "\n",
    "While it is possible to define a custom beta schedule, it is important to ensure that the noise follows the assumptions required for **sampling from an isotropic Gaussian**, particularly for $p_T$, the final step of the diffusion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jw2zpZGVAIC"
   },
   "outputs": [],
   "source": [
    "def cosine_beta_schedule(timesteps: int, s: float = 0.008) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (T).\n",
    "        s (float): Offset.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (timesteps,) containing the beta values.\n",
    "\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "\n",
    "def sigmoid_beta_schedule(timesteps: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the sigmoid beta schedule\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (T).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (timesteps,) containing the beta values.\n",
    "\n",
    "    \"\"\"\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-udjlGRVKbH8"
   },
   "source": [
    "#### Implementation of a quadratic beta schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NE5ispCEK0dQ"
   },
   "source": [
    "Although referred to as \"quadratic,\" this schedule does not perform a true quadratic interpolation. Instead, it is defined as:\n",
    "\n",
    "$$\n",
    "\\beta_t = \\left(\\sqrt{\\beta_{\\text{start}}} + \\left(\\sqrt{\\beta_{\\text{end}}} - \\sqrt{\\beta_{\\text{start}}}\\right) \\cdot \\frac{t}{T}\\right)^2\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $\\beta_{\\text{start}} $ is the initial beta value,  \n",
    "- $\\beta_{\\text{end}} $ is the final beta value,\n",
    "- $ T $ is the total number of timesteps,  \n",
    "- $ t $ is the current timestep $ t\\in [0, T]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : we can use the function:\n",
    "- `torch.linspace`: torch implementation similar to numpy linspace ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.linspace.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6D4Y4y1TE78Q"
   },
   "outputs": [],
   "source": [
    "# TODO implement the quadratic beta schedule.\n",
    "\n",
    "\n",
    "def quadratic_beta_schedule(timesteps: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the quadratic beta schedule.\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (T).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (timesteps,) containing the beta values.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2FWfP6lKftm"
   },
   "source": [
    "#### Implementation of a linear beta schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yO9yw2zLTKD"
   },
   "outputs": [],
   "source": [
    "# TODO implement the linear beta schedule\n",
    "\n",
    "\n",
    "def linear_beta_schedule(timesteps: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the linear beta schedule.\n",
    "\n",
    "    Args:\n",
    "        timesteps (int): Number of diffusion steps (T).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (timesteps,) containing the beta values.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkKxEN7XNFr4"
   },
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNX9-j8j-2dQ"
   },
   "outputs": [],
   "source": [
    "timesteps = 1001\n",
    "\n",
    "schedules = {\n",
    "    \"quadratic\": quadratic_beta_schedule,\n",
    "    \"linear\": linear_beta_schedule,\n",
    "    \"sigmoid\": sigmoid_beta_schedule,\n",
    "    \"cosine\": cosine_beta_schedule,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, len(schedules), figsize=(20, 5), sharex=True, sharey=False)\n",
    "\n",
    "for i, (name, schedule) in enumerate(schedules.items()):\n",
    "    beta = schedule(timesteps)\n",
    "    ax[i].plot(beta, label=name)\n",
    "    ax[i].set_title(name)\n",
    "    ax[i].set_xlabel(\"timestep\")\n",
    "\n",
    "ax[0].set_ylabel(\"beta\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAq9dhwiNSJe"
   },
   "source": [
    "#### Key takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx_vvWWkPam3"
   },
   "source": [
    "- The quadratic beta schedule ensures a smooth progression of noise variance over time.  \n",
    "- The interpolation is performed in **square-root space**, ensuring better control over noise scaling.  \n",
    "- This schedule prevents excessive noise in the early steps while maintaining enough variance for effective learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwAw1srRNydD"
   },
   "source": [
    "<a id=\"forward\"></a>\n",
    "### c. Forward process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdJptif7PPGp"
   },
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQuV5AXnW2oq"
   },
   "source": [
    "With the **beta schedule** now defined, we can move on to the implementation of the **forward diffusion process**.\n",
    "\n",
    "Recall the following transition kernel which adds noise to $\\mathbf{x}_{t-1}$ to obtain $\\mathbf{x}_t$:\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1 - \\beta(t)} \\mathbf{x}_{t-1}, \\beta(t) \\mathit{\\boldsymbol{I}}).\n",
    "$$\n",
    "\n",
    "We can efficiently generate a noisy version of the input at any timestep $t$ **without iterating through all previous timesteps** using the following:\n",
    "$$\n",
    "q(\\mathbf{x}_t \\mid \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathit{\\boldsymbol{I}}),\n",
    "$$\n",
    "\n",
    "where $\\bar{\\alpha}_t $ is the **cumulative product** of $(1 - \\beta_s)$ up to timestep $t$:\n",
    "  $$\n",
    "  \\bar{\\alpha}_t = \\prod_{s=1}^{t} (1 - \\beta_s).\n",
    "  $$\n",
    "\n",
    "Using the reparameterization trick, we can sample $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{\\epsilon}; \\mathbf{0}, \\mathit{\\boldsymbol{I}})$ and set  \n",
    "$$\n",
    "\\mathbf{x}_t = \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + (1 - \\bar{\\alpha}_t) \\epsilon.\n",
    "$$\n",
    "\n",
    "Note the following:\n",
    "- The mean $ \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 $ ensures that the signal decreases gradually over time,\n",
    "- The variance term $(1 - \\bar{\\alpha}_t) \\mathit{\\boldsymbol{I}} $ ensures that the noise component increases over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzZfzP2XN_hB"
   },
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : You can use the following functions:\n",
    "- `torch.randn_like` : torch function to generate Gaussian noise of the same size of the input ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.randn_like.html))\n",
    "- `torch.cumprod`: torch function to do the cumulative product ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.cumprod.html))\n",
    "- `.view()`: method to reshape a tensor ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.view.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpjvvGE0W-PB"
   },
   "outputs": [],
   "source": [
    "# TODO implement the forward process\n",
    "\n",
    "\n",
    "def q_sample(\n",
    "    x_0: torch.Tensor,\n",
    "    t: torch.Tensor,\n",
    "    betas: torch.Tensor,\n",
    "    noise: Optional[torch.Tensor] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the forward process to sample a noisy version of x_0 at timestep t.\n",
    "\n",
    "    Args:\n",
    "        x_0 (torch.Tensor): The original clean data (image or feature vector).\n",
    "        t (torch.Tensor): Timestep tensor (batch of time indices).\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "        noise (torch.Tensor) or None : Optional tensor of noise to be added. If None, generates noise randomly.\n",
    "\n",
    "    Returns:\n",
    "        x_t (torch.Tensor): Noisy version of x_0 at timestep t.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0, device=x_0.device)\n",
    "\n",
    "    betas = betas.to(x_0.device)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = torch.sqrt(alphas_cumprod[t]).view(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - alphas_cumprod[t]).view(\n",
    "        -1, 1, 1, 1\n",
    "    )\n",
    "\n",
    "    return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GilNwb20Qrmv"
   },
   "source": [
    "#### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08Qpbj7wog45"
   },
   "outputs": [],
   "source": [
    "# TODO: play along with the following visualisation code to compare the different beta schedules.\n",
    "\n",
    "timesteps = 1001\n",
    "\n",
    "schedules = {\n",
    "    \"quadratic\": quadratic_beta_schedule,\n",
    "    \"linear\": linear_beta_schedule,\n",
    "    \"sigmoid\": sigmoid_beta_schedule,\n",
    "    \"cosine\": cosine_beta_schedule,\n",
    "}\n",
    "\n",
    "# Timesteps to visualise\n",
    "test_steps = np.arange(0, 1001, 100)\n",
    "\n",
    "\n",
    "# Load data sample\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: torch.rot90(x, dims=(1, 2))),\n",
    "        transforms.Lambda(\n",
    "            lambda x: 2 * (x - x.min()) / (x.max() - x.min()) - 1\n",
    "        ),  # Normalize the image between -1 and 1,\n",
    "        transforms.CenterCrop((52, 52)),\n",
    "    ]\n",
    ")\n",
    "index = np.random.randint(0, 10)\n",
    "x_0 = IXIDataset(img_dir, mode=\"train\", transform=transform)[index][\"T1\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(schedules), len(test_steps), figsize=(40, 15))\n",
    "\n",
    "for i, (name, schedule) in enumerate(schedules.items()):\n",
    "    betas = schedule(timesteps)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "    for j, t in enumerate(test_steps):\n",
    "        x_t = q_sample(x_0, t, betas)\n",
    "        axes[i][j].imshow(x_t.squeeze().numpy(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "        axes[i][j].set_xticks([])\n",
    "        axes[i][j].set_yticks([])\n",
    "\n",
    "        if i == 0:\n",
    "            axes[i][j].set_title(f\"t = {t}\")\n",
    "\n",
    "        if j == 0:\n",
    "            axes[i][j].set_ylabel(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5O_zdK8xS6S2"
   },
   "outputs": [],
   "source": [
    "# TODO: play along with the following visualisation code\n",
    "\n",
    "timesteps = 1001\n",
    "\n",
    "schedule = quadratic_beta_schedule\n",
    "\n",
    "# Load data sample\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: torch.rot90(x, dims=(1, 2))),\n",
    "        transforms.Lambda(\n",
    "            lambda x: 2 * (x - x.min()) / (x.max() - x.min()) - 1\n",
    "        ),  # Normalize the image between -1 and 1,\n",
    "        transforms.CenterCrop((52, 52)),\n",
    "    ]\n",
    ")\n",
    "index = np.random.randint(0, 10)\n",
    "x_0 = IXIDataset(img_dir, mode=\"train\", transform=transform)[index][\"T1\"]\n",
    "\n",
    "betas = schedule(timesteps)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for t in np.arange(0, timesteps, 5):\n",
    "    x_t = q_sample(x_0, t, betas)\n",
    "    imgs.append([plt.imshow(x_t.squeeze().numpy(), cmap=\"gray\", vmin=-1, vmax=1)])\n",
    "plt.close()\n",
    "ani = animation.ArtistAnimation(fig, imgs, interval=100, blit=True, repeat_delay=1000)\n",
    "\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQYiKuE9blW1"
   },
   "source": [
    "<a id=\"reverse\"></a>\n",
    "### d. Reverse process (from different perspectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgECZO6CrIv6"
   },
   "source": [
    "#### i. True data predictori\n",
    "\n",
    "Now, we will define the **data perspective** of the diffusion model by formulating its **training objective**.  \n",
    "\n",
    "**Defining the Loss Function**  \n",
    "\n",
    "To train the diffusion model, we use a **mean squared error (MSE) loss** that measures the difference between the original data **$ \\mathbf{x}_0 $** and the predicted reconstruction **$ \\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t, t) $**. This loss function, known as the **simplified loss** from Ho et al. (2020), is defined as:  \n",
    "\n",
    "\\begin{align}\n",
    "L_{simple} (\\theta) := \\mathbb{E}_{t,\\mathbf{x}_0,\\mathbf{x}_t} \\left[ \\| \\mathbf{x_0} - \\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t,t) \\|^2 \\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where:  \n",
    "- **$ \\mathbf{x}_0 $** is the original, clean data sample.  \n",
    "- **$ \\mathbf{x}_t $** is the noisy version of the data at timestep $ t $.  \n",
    "- **$ \\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t, t) $** is the model's prediction of the original data.  \n",
    "- **$ \\mathbb{E}[\\cdot] $** denotes expectation over the sampled timesteps and data points.  \n",
    "\n",
    "**Interpretation of the Loss**  \n",
    "\n",
    "- This objective encourages the network **$ \\hat{\\mathbf{x}}_{\\theta} $** to learn to reconstruct the original data **$ \\mathbf{x}_0 $** given a noisy input **$ \\mathbf{x}_t $** at any timestep $ t $.  \n",
    "- It is a **denoising objective**, meaning that the model gradually learns to remove noise and recover structure from noisy inputs.  \n",
    "- The expectation is computed over **random timesteps** and **data samples**, ensuring that the model generalizes well across different noise levels.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIg-3gQor88Y"
   },
   "outputs": [],
   "source": [
    "def p_loss_data(\n",
    "    denoise_model: nn.Module,\n",
    "    x_0: TensorType[\"batch\", \"channel\", \"width\", \"height\"],\n",
    "    t: TensorType[\"batch\"],\n",
    "    betas: TensorType[\"timesteps\"],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the data perspective loss function.\n",
    "\n",
    "    Args:\n",
    "        denoise_model (nn.Module): The denoising model.\n",
    "        x_0 (torch.Tensor): The original clean data (image or feature vector).\n",
    "        t (torch.Tensor): Timestep tensor (batch of time indices).\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "    Returns:\n",
    "        loss.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    noise = torch.randn_like(x_0, device=x_0.device)\n",
    "\n",
    "    x_t = q_sample(x_0=x_0, t=t, betas=betas, noise=noise)\n",
    "\n",
    "    ## reminder the network takes as input x_t  (batch, channels, width, height) and time t (batch,)\n",
    "\n",
    "    predicted_x0 = denoise_model(x_t, t)\n",
    "\n",
    "    return F.mse_loss(x_0, predicted_x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gd6twz6osSP9"
   },
   "source": [
    "**Training**  \n",
    "\n",
    "Now that we have defined the loss function, the next step is to create the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlGzY34zsKeh"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Lambda(lambda x: torch.rot90(x, dims=(1, 2))),\n",
    "        transforms.Lambda(\n",
    "            lambda x: 2 * (x - x.min()) / (x.max() - x.min()) - 1\n",
    "        ),  # Normalize the image between -1 and 1\n",
    "        transforms.CenterCrop((52, 52)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = IXIDataset(img_dir, mode=\"train\", transform=transform)\n",
    "\n",
    "val_dataset = IXIDataset(img_dir, mode=\"test\", transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYTuNGlHsW72"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "image_size = 52\n",
    "channels = 1\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 1, 2),\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9999)\n",
    "\n",
    "timesteps = 500\n",
    "betas = quadratic_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "epochs = 300  # minimum epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEUYm89IR6bX"
   },
   "source": [
    "**TO DO: complete the training function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint** : You can use the following functions:\n",
    "- `torch.randint` Returns a tensor filled with random integers generated uniformly between `low` and `high` ([documentation](https://docs.pytorch.org/docs/stable/generated/torch.randint.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nibVXzdb3iaD"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    p_loss,\n",
    "    betas,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=300,\n",
    "    model_type=\"data\",\n",
    "):\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"epochs\", leave=False):\n",
    "        train_loss = 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"batch\", leave=False)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = batch[\"T1\"].to(device).to(torch.float32)\n",
    "\n",
    "            batch_size = batch.shape[0]\n",
    "\n",
    "            # TODO\n",
    "\n",
    "            # generate the timestep TensorType\"batch\"] don't forget to set the device\n",
    "            # and the type to long\n",
    "\n",
    "            t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "            # compute the loss (p_loss) using the p_loss function (it will take into input the model, batch, t and betas)\n",
    "            loss = p_loss(model, batch, t, betas)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # until here\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            tqdm._instances.clear()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"train loss: {train_loss / len(train_dataloader)}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            img = model._sample_training(\n",
    "                (1, 1, *batch.shape[-2:]), timesteps, betas, device, model_type\n",
    "            )\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(img.cpu().squeeze().numpy(), cmap=\"gray\")\n",
    "            plt.title(f\"epoch {epoch}\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        torch.save(model.state_dict(), f\"models/model_{model_type}.pt\")\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            loss_val = 0\n",
    "            for step, batch in enumerate(\n",
    "                tqdm(val_dataloader, desc=\"validation\", leave=False)\n",
    "            ):\n",
    "                with torch.no_grad():\n",
    "                    batch = batch[\"T1\"].to(device).to(torch.float32)\n",
    "\n",
    "                    batch_size = batch.shape[0]\n",
    "\n",
    "                    # TODO\n",
    "\n",
    "                    # generate the timestep\n",
    "\n",
    "                    t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
    "\n",
    "                    loss = p_loss(model, batch, t, betas)\n",
    "\n",
    "            loss_val = loss_val / len(val_dataloader.dataset)\n",
    "            print(\"Loss val:\", loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQUbvsAO38HN"
   },
   "source": [
    "<span style=\"color:red\">Warning:</span> Training the network using the data loss is possible, but inefficient and time-consuming. Instead, we will use a pretrained model for sampling.\n",
    "\n",
    "<span style=\"color:red\">DO NOT TRAIN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7IjSQa43-ju"
   },
   "outputs": [],
   "source": [
    "# train(p_loss_data, betas, model, optimizer, scheduler, model_type=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnWo8IbL8r8Y"
   },
   "source": [
    "##### Discussion about the sampling procedure\n",
    "\n",
    "Now that we have trained our model, it is time to **sample from an isotropic Gaussian distribution** to generate new data.  \n",
    "\n",
    "**Reminder: The Role of the Network**  \n",
    "\n",
    "In our diffusion model, the goal of the network is to **learn the reverse process**, which allows us to gradually remove noise from a pure Gaussian sample until we recover a structured output.  \n",
    "\n",
    "The reverse process is defined as:  \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t),\\Sigma_q(t) \\right)\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\mathbf{x}_{t-1} $** is the denoised sample at the previous timestep.  \n",
    "- **$ \\mathbf{x}_t $** is the current noisy sample.  \n",
    "- **$ \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) $** is the predicted mean of the distribution.  \n",
    "- **$ \\Sigma_q(t) $** is the variance of the noise, controlling the randomness in the sampling process.  \n",
    "\n",
    "**Mean Prediction Formula**  \n",
    "\n",
    "The mean $ \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) $ is given by:  \n",
    "\n",
    "$$\n",
    "\\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) = \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1}) \\mathbf{x}_t + \\sqrt{\\bar{\\alpha}_{t-1}}(1 - \\alpha_{t}) \\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t,t)}{1 - \\bar{\\alpha}_{t}}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\alpha_t $** and **$ \\bar{\\alpha}_t $** are derived from the beta schedule.  \n",
    "- **$ \\hat{\\mathbf{x}}_{\\theta}(\\mathbf{x}_t,t) $** is the model's predicted reconstruction of the clean data.  \n",
    "- The equation represents a weighted combination of the noisy sample $ \\mathbf{x}_t $ and the predicted clean data, ensuring a smooth transition from noise to structure.  \n",
    "\n",
    "**Variance Term**  \n",
    "\n",
    "The variance **$ \\Sigma_q(t) $** is defined as:  \n",
    "\n",
    "$$\n",
    "\\Sigma_q(t) = \\sigma_q^2(t) \\mathit{\\boldsymbol I} = \\frac{(1 - \\alpha_{t})(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_{t}}  \\mathit{\\boldsymbol I}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\sigma_q^2(t) $** determines the noise level at each step.  \n",
    "- The term ensures that the noise is reduced gradually over time, stabilizing the sampling process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nniutKf681xi"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(\n",
    "    model: nn.Module,\n",
    "    x: TensorType[\"batch\", \"channel\", \"width\", \"height\"],\n",
    "    t: TensorType[\"batch\"],\n",
    "    betas: TensorType[\"timesteps\"],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the sampling step from x_t to x_{t-1}.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        x_t (torch.Tensor): The original clean data (image or feature vector).\n",
    "        t (torch.Tensor): Timestep tensor.\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "    Returns:\n",
    "        x_{t-1} (torch.Tensor): The denoised sample at the previous timestep.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    t = torch.tensor([t]).to(device)\n",
    "\n",
    "    betas = betas.to(device)\n",
    "\n",
    "    # Compute `alphas` as 1 minus `betas`\n",
    "    alphas = 1.0 - betas\n",
    "\n",
    "    # Compute the cumulative product of `alphas` along the specified axis\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "    # Alternative solutions for the cumprod of t-1\n",
    "\n",
    "    # Pad the cumulative product of `alphas` (excluding the last element) with 1.0 at the start\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "    # Other solution\n",
    "    alphas_cumprod_prev = torch.cat(\n",
    "        [torch.tensor([1.0], device=device), alphas_cumprod[:-1]]\n",
    "    )\n",
    "\n",
    "    # Compute the mean of the model output at time `t`\n",
    "    model_mean = (\n",
    "        torch.sqrt(alphas[t]) * (1 - alphas_cumprod[t]) * x\n",
    "        + torch.sqrt(alphas_cumprod_prev[t]) * (1 - alphas[t]) * model(x, t)\n",
    "    ) / (1 - alphas_cumprod[t])\n",
    "\n",
    "    # Compute the posterior variance\n",
    "    posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "\n",
    "    noise = torch.randn_like(x)\n",
    "\n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        return model_mean + torch.sqrt(posterior_variance[t]) * noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(\n",
    "    model: nn.Module,\n",
    "    shape: Tuple,\n",
    "    betas: TensorType[\"timesteps\"],\n",
    "    return_all: bool = False,\n",
    ") -> Union[torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Implements the sampling step from noise to x_0.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        shape (Tuple): noise shap (batch, channel, width, height).\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "        return_all (bool): Whether to return all samples timestep or just the last one.\n",
    "    Returns:\n",
    "        x_0 (torch.Tensor): The sample data.\n",
    "        if return_all is True, return a list of all samples timestep\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "\n",
    "    imgs = []\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    for i in tqdm(\n",
    "        reversed(range(0, len(betas))), desc=\"sampling loop timestep\", total=timesteps\n",
    "    ):\n",
    "        img = p_sample(model, img, i, betas)\n",
    "        if return_all:\n",
    "            imgs.append(img.cpu().numpy())\n",
    "    # until here\n",
    "\n",
    "    if return_all:\n",
    "        return imgs\n",
    "    else:\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it was not possible for you to train the network, set pretrained to True\n",
    "pretrained = True\n",
    "if pretrained:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    image_size = 52\n",
    "    channels = 1\n",
    "\n",
    "    model = Unet(\n",
    "        dim=image_size,\n",
    "        channels=channels,\n",
    "        dim_mults=(1, 1, 2),\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    timesteps = 500\n",
    "    betas = quadratic_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(\"model_teacher/model_data.pt\", map_location=device))\n",
    "\n",
    "    img = p_sample_loop(model, shape = (1,1,52,52), betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(img.squeeze().cpu().numpy(), cmap=\"gray\", vmin = -1, vmax =1)\n",
    "plt.title(\"Generated image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XBeTNC1XKs4"
   },
   "source": [
    "#### ii. Noise predictor\n",
    "\n",
    "Now, we will define the **noise perspective** of the diffusion model by formulating its **training objective**.  \n",
    "\n",
    "**Defining the Loss Function**  \n",
    "\n",
    "We use the reparameterization trick, we can rewrite the forward process as:\n",
    "\n",
    "\\begin{align}\n",
    "q(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}) & = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathit{\\boldsymbol{I}}) \\\\\n",
    " \\mathbf{x}_{t}&  =  \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_{0} +  \\sqrt{(1 - \\bar{\\alpha}_t)} \\boldsymbol \\epsilon_0, \\quad \\boldsymbol \\epsilon_0 \\sim \\mathcal{N}(\\mathbf{0},\\mathit{\\boldsymbol I})\n",
    "\\end{align}\n",
    "\n",
    "So we have:\n",
    "$$\n",
    "\\mathbf{x}_{0} = \\frac{\\mathbf{x}_{t} -  \\sqrt{(1 - \\bar{\\alpha}_t)} \\boldsymbol \\epsilon_0 }{\\sqrt{\\bar{\\alpha}_t}}\n",
    "$$\n",
    "\n",
    "The network only needs to predict $ \\boldsymbol \\epsilon_0$ to predict $\\mathbf{x}_{0}$. The network prediction is $\\hat{\\boldsymbol\\epsilon}_{\\theta}(\\mathbf{x_t},t)$.\n",
    "\n",
    "To train the diffusion model, we use a **mean squared error (MSE) loss** that measures the difference between the original input noise $\\boldsymbol \\epsilon_0 \\sim \\mathcal{N}(\\mathbf{0},\\mathit{\\boldsymbol I})$ and the predicted noise $\\hat{\\boldsymbol\\epsilon}_{\\theta}(\\mathbf{x_t},t)$.\n",
    "\n",
    "\n",
    "This loss function, known as the **simplified loss** from Ho et al. (2020), is defined as:  \n",
    "\n",
    "\\begin{align}\n",
    "L_{simple} (\\theta) := \\mathbb{E}_{t,\\mathbf{x}_0, \\boldsymbol\\epsilon_0} \\left[ \\| \\boldsymbol \\epsilon_0  - \\hat{\\boldsymbol\\epsilon}_{\\theta}(\\mathbf{x_t},t) \\|^2 \\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where:  \n",
    "- **$ \\mathbf{x}_0 $** is the original, clean data sample.  \n",
    "- **$ \\mathbf{x}_t $** is the noisy version of the data at timestep $ t $.  \n",
    "- **$ \\boldsymbol \\epsilon_{\\theta}(\\mathbf{x_t},t) $** is the model's prediction of the input noise applied to $\\mathbf{x}_0$.  \n",
    "- **$ \\mathbb{E}[\\cdot] $** denotes expectation over the sampled timesteps, data points and noises.  \n",
    "\n",
    "**Interpretation of the Loss**  \n",
    "\n",
    "- This objective encourages the network $ \\hat{\\boldsymbol\\epsilon}_{\\theta} $ to learn to reconstruct the original input noise $\\boldsymbol \\epsilon_0 $ utilized in the forward process to sample $ \\mathbf{x}_t $ at any timestep $ t $.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uf_V0jGJXSCT"
   },
   "outputs": [],
   "source": [
    "def p_loss_noise(\n",
    "    model: nn.Module,\n",
    "    x_0: TensorType[\"batch\", \"channel\", \"width\", \"height\"],\n",
    "    t: TensorType[\"batch\"],\n",
    "    betas: TensorType[\"timesteps\"],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the noise perspective loss function.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The denoising model.\n",
    "        x_0 (torch.Tensor): The original clean data (image or feature vector).\n",
    "        t (torch.Tensor): Timestep tensor (batch of time indices).\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "    Returns:\n",
    "        loss.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    noise = torch.randn_like(x_0, device=x_0.device)\n",
    "\n",
    "    x_t = q_sample(x_0=x_0, t=t, betas=betas, noise=noise)\n",
    "    predicted_noise = model(x_t, t)\n",
    "\n",
    "    return F.mse_loss(noise, predicted_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BS1aZVqrfsY"
   },
   "source": [
    "**Training**\n",
    "\n",
    "We defined the architecture, the betas schedule, the forward process to sample noisy version of the data and the loss. We can now define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MB_mgTejrzey"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "image_size = 52\n",
    "channels = 1\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 1, 2),\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = Adam(\n",
    "    model.parameters(), lr=1e-4\n",
    ")  # , betas=(0.9, 0.999),weight_decay = 0.01 )\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9999)\n",
    "\n",
    "timesteps = 500\n",
    "betas = quadratic_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "epochs = 300  # minimum epochs\n",
    "\n",
    "train(\n",
    "    p_loss_noise,\n",
    "    betas=betas,\n",
    "    model_type=\"noise\",\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn_HQlvoEDeL"
   },
   "source": [
    "##### Noise Sampling\n",
    "\n",
    "Now that we have trained our model, it is time to **sample from an isotropic Gaussian distribution** to generate new data.  \n",
    "\n",
    "**Reminder: The Role of the Network**  \n",
    "\n",
    "In our diffusion model, the goal of the network is to **learn the reverse process**, which allows us to gradually remove noise from a pure Gaussian sample until we recover a structured output.  \n",
    "\n",
    "The reverse process is defined as:  \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t),\\Sigma_q(t) \\right)\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\mathbf{x}_{t-1} $** is the denoised sample at the previous timestep.  \n",
    "- **$ \\mathbf{x}_t $** is the current noisy sample.  \n",
    "- **$ \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) $** is the predicted mean of the distribution.  \n",
    "- **$ \\Sigma_q(t) $** is the variance of the noise, controlling the randomness in the sampling process.  \n",
    "\n",
    "**Mean Prediction Formula**  \n",
    "\n",
    "The mean $ \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) $ is given by:  \n",
    "\n",
    "$$\n",
    "\\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) = \\frac{1}{\\sqrt{\\alpha_t}} \\mathbf{x}_t - \\frac{1 - \\alpha_{t}}{\\sqrt{1 - \\bar{\\alpha}_{t}}\\sqrt{\\alpha_t}} \\epsilon_{\\theta}(\\mathbf{x_t},t)\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\alpha_t $** and **$ \\bar{\\alpha}_t $** are derived from the beta schedule.  \n",
    "- **$ \\epsilon_{\\theta}(\\mathbf{x_t},t) $** is the model's predicted noise to retreive the clean data.  \n",
    "- The equation represents a weighted combination of the noisy sample $ \\mathbf{x}_t $ and the predicted noise to retreive the clean data, ensuring a smooth transition from noise to structure.  \n",
    "\n",
    "**Variance Term**  \n",
    "\n",
    "The variance **$ \\Sigma_q(t) $** is defined as:  \n",
    "\n",
    "$$\n",
    "\\Sigma_q(t) = \\sigma_q^2(t) \\mathit{\\boldsymbol I} = \\frac{(1 - \\alpha_{t})(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_{t}}  \\mathit{\\boldsymbol I}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\sigma_q^2(t) $** determines the noise level at each step.  \n",
    "- The term ensures that the noise is reduced gradually over time, stabilizing the sampling process.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkTBnt_SuE3C"
   },
   "outputs": [],
   "source": [
    "# If it was not possible for you to train the network, set pretrained to True\n",
    "pretrained = False\n",
    "if pretrained:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    image_size = 52\n",
    "    channels = 1\n",
    "\n",
    "    model = Unet(\n",
    "        dim=image_size,\n",
    "        channels=channels,\n",
    "        dim_mults=(1, 1, 2),\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    timesteps = 500\n",
    "    betas = quadratic_beta_schedule(timesteps=timesteps)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(\"model_teacher/model_noise.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFaaTHtfF5CZ"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(\n",
    "    model: nn.Module,\n",
    "    x: TensorType[\"batch\", \"channel\", \"width\", \"height\"],\n",
    "    t: TensorType[\"batch\"],\n",
    "    betas: TensorType[\"timesteps\"],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the sampling step from x_t to x_{t-1}.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        x_t (torch.Tensor): The original clean data (image or feature vector).\n",
    "        t (torch.Tensor): Timestep tensor.\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "    Returns:\n",
    "        x_{t-1} (torch.Tensor): The denoised sample at the previous timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    t = torch.tensor([t]).to(device)\n",
    "    betas = betas.to(device)\n",
    "\n",
    "    # Compute `alphas` as 1 minus `betas`\n",
    "    alphas = 1.0 - betas\n",
    "\n",
    "    # Compute the cumulative product of `alphas` along the specified axis\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "    # Compute the square root of the reciprocal of `alphas`\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "    # Compute the square root of 1 minus the cumulative product of `alphas` at time `t`\n",
    "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - alphas_cumprod[t])\n",
    "\n",
    "    # Get the value of `sqrt_recip_alphas` at time `t`\n",
    "    sqrt_recip_alphas_t = sqrt_recip_alphas[t]\n",
    "\n",
    "    # Compute the mean of the model output at time `t`\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas[t] * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "\n",
    "    # Pad the cumulative product of `alphas` (excluding the last element) with 1.0 at the start\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "    # Compute the posterior variance\n",
    "    posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "\n",
    "    noise = torch.randn_like(x)\n",
    "\n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        return model_mean + torch.sqrt(posterior_variance[t]) * noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(\n",
    "    model: nn.Module,\n",
    "    shape: Tuple,\n",
    "    betas: TensorType[\"timesteps\"],\n",
    "    return_all: bool = False,\n",
    ") -> Union[torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Implements the sampling step from noise to x_0.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        shape (Tuple): noise shap (batch, channel, width, height).\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "        return_all (bool): Whether to return all samples timestep or just the last one.\n",
    "    Returns:\n",
    "        x_0 (torch.Tensor): The sample data.\n",
    "        if return_all is True, return a list of all samples timestep\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "\n",
    "    imgs = []\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    for i in tqdm(\n",
    "        reversed(range(0, len(betas))), desc=\"sampling loop timestep\", total=timesteps\n",
    "    ):\n",
    "        img = p_sample(model, img, i, betas)\n",
    "        if return_all:\n",
    "            imgs.append(img.cpu().numpy())\n",
    "\n",
    "    if return_all:\n",
    "        return imgs\n",
    "    else:\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrokpdB2LyXT"
   },
   "outputs": [],
   "source": [
    "img = p_sample_loop(model, shape = (10,1,52,52), betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bSShEWckxV3"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5.0, 5.0))\n",
    "\n",
    "grid = ImageGrid(\n",
    "    fig,\n",
    "    111,  # similar to subplot(111)\n",
    "    nrows_ncols=(3, 3),  # creates 2x2 grid of Axes\n",
    "    axes_pad=0.1,  # pad between Axes in inch.\n",
    ")\n",
    "\n",
    "for i, ax in enumerate(grid):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(img[i].cpu().squeeze(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF1_xaUxdarz"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ims = []\n",
    "\n",
    "img = p_sample_loop(model, shape = (1,1,52,52), betas = betas, return_all=True)\n",
    "\n",
    "\n",
    "for img in imgs:\n",
    "    ims.append([plt.imshow(img.squeeze(), cmap=\"gray\", vmin=-1, vmax=1)])\n",
    "\n",
    "plt.close()\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cOnFnDze7Yl"
   },
   "outputs": [],
   "source": [
    "ani.save(\"reverse_process.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytrjS_tzKarG"
   },
   "source": [
    "#### iii. Score predictor\n",
    "\n",
    "Now, we will define the **score perspective** of the diffusion model by formulating its **training objective**.  \n",
    "\n",
    "**Defining the Loss Function**  \n",
    "\n",
    "Applying the Tweedie's formula on the forward process $q(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t) \\mathit{\\boldsymbol{I}})$ :\n",
    "\n",
    "$$\n",
    "\\mathbb{E} [\\boldsymbol \\mu_{\\mathbf{x}_t} \\mid \\mathbf{x}_{t}] = \\mathbf{x}_{t} + (1-\\bar{\\alpha}_t) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)\n",
    "$$\n",
    "\n",
    "Thus we have:\n",
    "$$\n",
    " \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 = \\mathbf{x}_{t} + (1-\\bar{\\alpha}_t) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)\n",
    "$$\n",
    "\n",
    "and finally:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{0} = \\frac{\\mathbf{x}_{t} +  (1 - \\bar{\\alpha}_t) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t) }{\\sqrt{\\bar{\\alpha}_t}}\n",
    "$$\n",
    "\n",
    "The network only need to predict the score $ \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t) $ to predict $\\mathbf{x}_{0}$. The network prediction is $\\hat{\\mathbf{s}}_{\\theta}(\\mathbf{x_t},t)$.\n",
    "\n",
    "To train the diffusion model, we use a **mean squared error (MSE) loss** that measures the difference between the true score $\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t) $ and the predicted score $\\hat{\\mathbf{s}}_{\\theta}(\\mathbf{x_t},t)$.\n",
    "\n",
    "The loss is then:   \n",
    "\n",
    "$$\n",
    "L_{\\text{score}} (\\theta) := \\mathbb{E}_{t,\\mathbf{x}_0, \\boldsymbol\\epsilon} (1- \\bar{\\alpha}_t) \\left[ \\| - \\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol \\epsilon_0 - \\hat{\\mathbf{s}}_{\\theta}(\\mathbf{x_t},t) \\|^2 \\right]\n",
    "$$  \n",
    "\n",
    "\n",
    "where:  \n",
    "- $ \\mathbf{x}_0 $ is the original, clean data sample.  \n",
    "- $ \\mathbf{x}_t $ is the noisy version of the data at timestep $ t $.  \n",
    "- $ \\hat{\\mathbf{s}}_{\\theta}(\\mathbf{x_t},t) $ is the model's prediction of the score.  \n",
    "- $ \\mathbb{E}[\\cdot] $ denotes expectation over the sampled timesteps, data points and noises.    \n",
    "\n",
    "But how to compute $\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t)$, if you look at the noise predictor diffusion model we can see that you have a relation between the input noise and the score\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t) = - \\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol \\epsilon_0\n",
    "\\end{align}\n",
    "\n",
    "**But we just trained before the noise predictor point of view. So we don't need to retrain the model. We can directly use the previous equation for the sampling.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljGdCu3kBUPv"
   },
   "source": [
    "##### Score Sampling  \n",
    "\n",
    "Now that we have trained our model, it is time to **sample from an isotropic Gaussian distribution** to generate new data.  \n",
    "\n",
    "**Reminder: The Role of the Network**  \n",
    "\n",
    "In our diffusion model, the goal of the network is to **learn the reverse process**, which allows us to gradually remove noise from a pure Gaussian sample until we recover a structured output.  \n",
    "\n",
    "The reverse process is defined as:  \n",
    "\n",
    "$$\n",
    "p_{\\theta}(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_t) = \\mathcal{N} \\left(\\mathbf{x}_{t-1}; \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t),\\Sigma_q(t) \\right)\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\mathbf{x}_{t-1} $** is the denoised sample at the previous timestep.  \n",
    "- **$ \\mathbf{x}_t $** is the current noisy sample.  \n",
    "- **$ \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) $** is the predicted mean of the distribution.  \n",
    "- **$ \\Sigma_q(t) $** is the variance of the noise, controlling the randomness in the sampling process.  \n",
    "\n",
    "**Mean Prediction Formula**  \n",
    "\n",
    "The mean $ \\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) $ is given by:  \n",
    "\n",
    "$$\n",
    "\\boldsymbol \\mu_{\\theta}(\\mathbf{x}_{t},t) = \\frac{1}{\\sqrt{\\alpha_t}} \\mathbf{x}_t + \\frac{1 - \\alpha_{t}}{\\sqrt{\\alpha_t}} \\mathbf{s}_{\\theta}(\\mathbf{x_t},t)\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\alpha_t $** and **$ \\bar{\\alpha}_t $** are derived from the beta schedule.  \n",
    "- **$ \\mathbf{s}_{\\theta}(\\mathbf{x_t},t) $** is the model's predicted of the score (e.g $\\nabla \\log p(\\mathbf{x}_t)$).\n",
    "- The equation represents a weighted combination of the noisy sample $ \\mathbf{x}_t $ and the score, ensuring a smooth transition from noise to structure.  \n",
    "\n",
    "**Variance Term**  \n",
    "\n",
    "The variance **$ \\Sigma_q(t) $** is defined as:  \n",
    "\n",
    "$$\n",
    "\\Sigma_q(t) = \\sigma_q^2(t) \\mathit{\\boldsymbol I} = \\frac{(1 - \\alpha_{t})(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_{t}}  \\mathit{\\boldsymbol I}\n",
    "$$  \n",
    "\n",
    "where:  \n",
    "- **$ \\sigma_q^2(t) $** determines the noise level at each step.  \n",
    "- The term ensures that the noise is reduced gradually over time, stabilizing the sampling process.  \n",
    "\n",
    "\n",
    "It was just an introduction to the score but it opens the road to the Score based model using Stochastic Differential Equation. [Song et al, 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEh5BHBGKGGz"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(\n",
    "    model: nn.Module,\n",
    "    x: TensorType[\"batch\", \"channel\", \"width\", \"height\"],\n",
    "    t: TensorType[\"batch\"],\n",
    "    betas: TensorType[\"timesteps\"],\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Implements the sampling step from x_t to x_{t-1}.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        x_t (torch.Tensor): The original clean data (image or feature vector).\n",
    "        t (torch.Tensor): Timestep tensor.\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "    Returns:\n",
    "        x_{t-1} (torch.Tensor): The denoised sample at the previous timestep.\n",
    "    \"\"\"\n",
    "    # TO DO\n",
    "\n",
    "    t = torch.tensor([t]).to(device)\n",
    "    betas = betas.to(device)\n",
    "\n",
    "    # Compute `alphas` as 1 minus `betas`\n",
    "    alphas = 1.0 - betas\n",
    "\n",
    "    # Compute the cumulative product of `alphas` along the specified axis\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "    # Compute the square root of the reciprocal of `alphas`\n",
    "    sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "    # Compute the square root of 1 minus the cumulative product of `alphas` at time `t`\n",
    "    sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - alphas_cumprod[t])\n",
    "\n",
    "    # Get the value of `sqrt_recip_alphas` at time `t`\n",
    "    sqrt_recip_alphas_t = sqrt_recip_alphas[t]\n",
    "\n",
    "    # compute the score:\n",
    "    eps = model(x, t)\n",
    "    score = -eps / sqrt_one_minus_alphas_cumprod_t\n",
    "\n",
    "    # Compute the mean of the model output at time `t`\n",
    "    model_mean = sqrt_recip_alphas_t * (x + betas[t] * score)\n",
    "\n",
    "    # Pad the cumulative product of `alphas` (excluding the last element) with 1.0 at the start\n",
    "    alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "    # Compute the posterior variance\n",
    "    posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "\n",
    "    noise = torch.randn_like(x)\n",
    "\n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        return model_mean + torch.sqrt(posterior_variance[t]) * noise\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(\n",
    "    model: nn.Module,\n",
    "    shape: Tuple,\n",
    "    betas: TensorType[\"timesteps\"],\n",
    "    return_all: bool = False,\n",
    ") -> Union[torch.Tensor, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Implements the sampling step from noise to x_0.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model.\n",
    "        shape (Tuple): noise shap (batch, channel, width, height).\n",
    "        betas (torch.Tensor): Precomputed beta schedule.\n",
    "        return_all (bool): Whether to return all samples timestep or just the last one.\n",
    "    Returns:\n",
    "        x_0 (torch.Tensor): The sample data.\n",
    "        if return_all is True, return a list of all samples timestep\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    b = shape[0]\n",
    "    # start from pure noise (for each example in the batch)\n",
    "    img = torch.randn(shape, device=device)\n",
    "\n",
    "    imgs = []\n",
    "\n",
    "    for i in tqdm(\n",
    "        reversed(range(0, timesteps)), desc=\"sampling loop timestep\", total=timesteps\n",
    "    ):\n",
    "        img = p_sample(model, img, i, betas)\n",
    "        if return_all:\n",
    "            imgs.append(img.cpu().numpy())\n",
    "\n",
    "    if return_all:\n",
    "        return imgs\n",
    "    else:\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Sjax7j5u1t3"
   },
   "outputs": [],
   "source": [
    "img = p_sample_loop(model, shape = (10,1,52,52), betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgwS_2egu0Zf"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5.0, 5.0))\n",
    "\n",
    "grid = ImageGrid(\n",
    "    fig,\n",
    "    111,  # similar to subplot(111)\n",
    "    nrows_ncols=(3, 3),  # creates 3x3 grid of Axes\n",
    "    axes_pad=0.1,  # pad between Axes in inch.\n",
    ")\n",
    "\n",
    "for i, ax in enumerate(grid):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(img[i].cpu().squeeze(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjkqLaindBa2"
   },
   "source": [
    "<a id=\"anomdetect\"></a>\n",
    "## 4. Application to anomaly detection with anoDDPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to use a diffusion model to solve an unsupervised medical anomaly detection task. \n",
    "We will be working with brain FLAIR (fluid attenuated inversion recovery) MRI from the BraTS dataset, attempting to detect tumors. \n",
    "In terms of model, we will be recreating the famous AnoDDPM, a diffusion model specifically designed for medical anomaly detection. \n",
    "\n",
    "**Unsupervised anomaly detection via pseudo-healthy reconstruction**\n",
    "\n",
    "We will be doing unsupervised anomaly detection, meaning that we will learn to detect anomalies from unlabelled data, i.e. no segmentation masks, just a single label per image indicating whether the image is normal or abnormal.\n",
    "The idea is to learn the distribution of normal data by training our diffusion model only with healthy data. \n",
    "Once the model is trained, at inference time, we can give as input to our model any image (pathological or not), and the model will output a pseudo-healthy version of that image. \n",
    "By comparing the real image with its pseudo-healthy version outputted by the model, for example by looking at the difference between the two images, we obtain an anomaly map from which we can precisely localise anomalies. \n",
    "\n",
    "**AnoDDPM**\n",
    "\n",
    "AnoDDPM is a denoising diffusion probabilistic model (DDPM) with simplex noise (instead of isotropic Gaussian noise). \n",
    "You might be interested in the following links: \n",
    "- an overview of the framework: https://julianwyatt.co.uk/anoddpm, \n",
    "- the official code repository: https://github.com/Julian-Wyatt/AnoDDPM, \n",
    "- and the CVPR workshop 2022 paper: https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Wyatt_AnoDDPM_Anomaly_Detection_With_Denoising_Diffusion_Probabilistic_Models_Using_Simplex_CVPRW_2022_paper.html. \n",
    "\n",
    "**Data splitting**\n",
    "\n",
    "We need to separate data between normal (or healthy) for training and abnormal (or pathological) for both training and testing. \n",
    "Since all images in BraTS are pathological, we divide between healthy slices and pathological slices.\n",
    "To avoid data leakage, we ensure that all slices for a given subject are either in the training set or in the testing set. \n",
    "\n",
    "Note, separating slices between pathological and healthy is not good practice for the following reason: \n",
    "- slices labelled as healthy might not be truly healthy, for instance, if located close to a tumor, the tissue might be deformed,\n",
    "- healthy slices tend to be more frequent towards the edge of the brain, and pathological slices more frequent towards the center of the brain, inducing a localisation bias.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "We will be evaluating the following: \n",
    "1. Given a healthy image, the model outputs a healthy image, that closely ressembles the input. We evaluate this using similarity metrics such as the [Structural Similarity Index Measure](https://en.wikipedia.org/wiki/Structural_similarity_index_measure) (Wang et al., 2004)[https://ieeexplore.ieee.org/document/1284395], and the mean squared error (MSE). \n",
    "2. Given an abnormal image, we check that the anomaly map is close to the tumor segmentation mask. Since the anomaly maps are unthresholded, we will use the pixelwise Average Precision (AP) and the best possible Dice over a range of possible thresholds ($\\lceil \\text{Dice} \\rceil$). \n",
    "\n",
    "**BraTS**\n",
    "\n",
    "Our dataset comes from the [Brain Tumor Segmentation Challenge (BraTS)](https://www.cancerimagingarchive.net/analysis-result/rsna-asnr-miccai-brats-2021/) from 2021, generated by the TCGA Research Network: http://cancergenome.nih.gov/ and obtained as part of the RSNA-ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge project through Synapse ID (syn25829067). \n",
    "\n",
    "For more information regarding the dataset and the challenge, you can consult the following papers: \n",
    "\n",
    "- [1] U.Baid, et al., The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification, 2021, [(arXiv:2107.02314)](https://arxiv.org/abs/2107.02314).\n",
    "- [2] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et al. \"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)\", IEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) [DOI: 10.1109/TMI.2014.2377694](https://doi.org/10.1109/tmi.2014.2377694).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCXeAqqHs0Xu"
   },
   "outputs": [],
   "source": [
    "root_dir = \"BraTS2021\"\n",
    "\n",
    "abnormal_dataset = Subset(\n",
    "    BraTSDataset(\n",
    "        root_dir,\n",
    "        mode=\"test\",\n",
    "        test_type=\"abnormal\",\n",
    "        transform=transforms.Lambda(\n",
    "            lambda x: 2 * (x - x.min()) / (x.max() - x.min()) - 1\n",
    "        ),\n",
    "    ),\n",
    "    range(30),\n",
    ")\n",
    "\n",
    "abnormal_sample = abnormal_dataset[60]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "axes[0].imshow(abnormal_sample[\"img\"].squeeze(), cmap=\"gray\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"Abnormal scan\")\n",
    "\n",
    "axes[1].imshow(abnormal_sample[\"mask\"].squeeze(), cmap=\"gray\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Anomaly segmentation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czERIywRjpXs"
   },
   "source": [
    "### Simplex noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BaLNP4QaNnf"
   },
   "source": [
    "**Perlin/Simplex** noise is a smooth, pseudo-random function invented by Ken Perlin. It is used to generate natural-looking textures (e.g., clouds, terrain).\n",
    "\n",
    "The difference between Perlin and Simplex is that Simplex uses triangles lattices in 2D rather than Perlin that uses square lattices. \n",
    "In order to make the computation faster for this lab session, we will use Perlin noise.\n",
    "\n",
    "This noise can be seen as “randomness with memory”, it changes smoothly rather than abruptly like Gaussian noise.\n",
    "It is a gradient-based noise: instead of assigning random values to grid points, it assigns random gradients (directions), and interpolates between them.\n",
    "\n",
    "The parameters are:\n",
    "\n",
    "- **octaves**: number of noise layers (frequencies) summed (more octaves means more details)\n",
    "- **persistence**: controls the amplitude reduction between the octaves\n",
    "\n",
    "It has been shown in practice that this noise produces better results than Gaussian noise on medical imaging, since it looks closer to real-world anomalies that we find in medical imaging data.\n",
    "\n",
    "It might be worth noting, that at the beginning of this practical session, we mentionned the following two assumptions: \n",
    "- $x_T$ must be close to an isotropic Gaussian distribution, \n",
    "- and each forward step is Gaussian . \n",
    "\n",
    "Using a simplex noise violates both of these assumptions, yet empirically, everything works anyways! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZ6PDgUz0FpG"
   },
   "outputs": [],
   "source": [
    "x = torch.randn((1, 1, 128, 128))\n",
    "t = torch.rand((1,))\n",
    "\n",
    "simplex_noise = rand_perlin_2d_octaves(x.shape, res=(1, 1), octaves=6, persistence=0.8)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].imshow(torch.randn((128, 128)).squeeze().numpy())\n",
    "axes[0].set_title(\"Gaussian Noise\")\n",
    "\n",
    "axes[1].imshow(simplex_noise.squeeze().numpy())\n",
    "axes[1].set_title(\"Perlin Noise\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFdMFR-xjJYE"
   },
   "outputs": [],
   "source": [
    "# TODO: play along with the following visualisation code to compare the different beta schedules.\n",
    "\n",
    "timesteps = 1001\n",
    "\n",
    "schedules = {\n",
    "    \"quadratic\": quadratic_beta_schedule,\n",
    "    \"linear\": linear_beta_schedule,\n",
    "    \"sigmoid\": sigmoid_beta_schedule,\n",
    "    \"cosine\": cosine_beta_schedule,\n",
    "}\n",
    "\n",
    "# Timesteps to visualise\n",
    "test_steps = np.arange(0, 1001, 100)\n",
    "\n",
    "\n",
    "index = np.random.randint(0, 10)\n",
    "x_0 = abnormal_sample[\"img\"].unsqueeze(0)\n",
    "\n",
    "fig, axes = plt.subplots(len(schedules), len(test_steps), figsize=(40, 15))\n",
    "\n",
    "for i, (name, schedule) in enumerate(schedules.items()):\n",
    "    betas = schedule(timesteps)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "    for j, t in enumerate(test_steps):\n",
    "        x_t = q_sample(\n",
    "            x_0,\n",
    "            t,\n",
    "            betas,\n",
    "            noise=rand_perlin_2d_octaves(\n",
    "                x_0.shape, res=(1, 1), octaves=8, persistence=0.8\n",
    "            ),\n",
    "        )\n",
    "        axes[i][j].imshow(x_t.squeeze().numpy(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "        axes[i][j].set_xticks([])\n",
    "        axes[i][j].set_yticks([])\n",
    "\n",
    "        if i == 0:\n",
    "            axes[i][j].set_title(f\"t = {t}\")\n",
    "\n",
    "        if j == 0:\n",
    "            axes[i][j].set_ylabel(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfU_H9sJjyGn"
   },
   "source": [
    "### Putting everything together\n",
    "\n",
    "We will be putting everything together in a single object class called `AnoDDPMDiffusionModel` that contains methods to perform the forward pass, the sampling and compute the loss. \n",
    "For the loss we will use the noise predictor. \n",
    "\n",
    "We will have to do the following steps:\n",
    "- implement the forward process method `q_sample` (use similar functions as in previous sections, nothing changes apart from the noise), \n",
    "- implement the `noise_loss` method which will be use for the training,\n",
    "- and implement the sampling part, using the method `p_sample`. \n",
    "\n",
    "If we were to generate new data samples (as in typical generative modelling frameworks), we would use `p_sample` from pure Simplex noise. \n",
    "In our case, we are interested in obtaining a pseudo-healthy reconstruction for a given specific image $x_0$, so we noise the image up to given a timestep $\\lambda$ (to be determined) to obtain $x_\\lambda$, which we then give as input to the reverse process through `p_sample` to obtain a subject-specific pseudo-healthy reconstruction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRLB-cZMj3KW"
   },
   "outputs": [],
   "source": [
    "class AnoDDPMDiffusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple diffusion model that can be used to train and sample from a diffusion process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, Unet, timesteps: int = 500, beta_fn=None, noise_type: str = \"gauss\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = Unet\n",
    "        self.timesteps = timesteps\n",
    "        self.beta_fn = beta_fn\n",
    "        self.noise_type = noise_type\n",
    "\n",
    "        if beta_fn is None:\n",
    "            self.betas = quadratic_beta_schedule(self.timesteps)\n",
    "        else:\n",
    "            self.betas = betas\n",
    "\n",
    "        if self.noise_type == \"gauss\":\n",
    "            self.noise_fn = lambda x: torch.randn_like(x, device=x.device)\n",
    "        elif self.noise_type == \"simplex\":\n",
    "            self.noise_fn = lambda x: rand_perlin_2d_octaves(\n",
    "                shape=x.shape, res=(1, 1), octaves=8, persistence=0.8\n",
    "            ).to(x.device)\n",
    "\n",
    "    def q_sample(\n",
    "        self,\n",
    "        x_0: TensorType[\"batch\", \"channel\", \"width\", \"height\"],\n",
    "        t: TensorType[\"batch\"],\n",
    "        noise: Optional[torch.Tensor] = None,\n",
    "    ) -> TensorType[\"batch\", \"channel\", \"width\", \"height\"]:\n",
    "        \"\"\"\n",
    "        Samples a noisy version of x_0 at timestep t.\n",
    "\n",
    "        Args:\n",
    "            x_0 (torch.Tensor): The original clean data (image or feature vector).\n",
    "            t (torch.Tensor): Timestep tensor (batch of time indices).\n",
    "            noise (torch.Tensor) or None : Optional tensor of noise to be added. If None, generates noise randomly.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Noisy version of x_0 at timestep t.\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "        if noise is None:\n",
    "            noise = self.noise_fn(x_0)\n",
    "\n",
    "        betas = self.betas.to(x_0.device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        sqrt_alphas_cumprod_t = torch.sqrt(alphas_cumprod[t]).view(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - alphas_cumprod[t]).view(\n",
    "            -1, 1, 1, 1\n",
    "        )\n",
    "\n",
    "        return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    def noise_loss(self, x_0: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO\n",
    "\n",
    "        t = torch.randint(0, self.timesteps, (x_0.size(0),), device=x_0.device).long()\n",
    "        noise = self.noise_fn(x_0)\n",
    "        x_t = self.q_sample(x_0=x_0, t=t, noise=noise)\n",
    "        predicted_noise = self.network(x_t, t)\n",
    "\n",
    "        return F.mse_loss(noise, predicted_noise)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x_t: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the diffusion model at a specific timestep.\n",
    "\n",
    "        Args:\n",
    "            x_t (torch.Tensor): The noisy data at timestep t.\n",
    "            t (int): The current timestep.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The denoised sample at the previous timestep.\n",
    "        \"\"\"\n",
    "        t = torch.tensor([t]).to(x_t.device)\n",
    "\n",
    "        # TODO\n",
    "\n",
    "        betas = self.betas.to(x_t.device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "        # Compute the cumulative product of `alphas` along the specified axis\n",
    "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "\n",
    "        # Compute the square root of the reciprocal of `alphas`\n",
    "        sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "        sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1.0 - alphas_cumprod[t])\n",
    "\n",
    "        # Get the value of `sqrt_recip_alphas` at time `t`\n",
    "        sqrt_recip_alphas_t = sqrt_recip_alphas[t]\n",
    "\n",
    "        # Compute the mean of the model output at time `t`\n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x_t - betas[t] * self.network(x_t, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "\n",
    "        # Pad the cumulative product of `alphas` (excluding the last element) with 1.0 at the start\n",
    "        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "\n",
    "        # Compute the posterior variance\n",
    "        posterior_variance = (\n",
    "            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        )\n",
    "\n",
    "        noise = self.noise_fn(x_t)\n",
    "\n",
    "        if t == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            return model_mean + torch.sqrt(posterior_variance[t]) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, x_0: torch.Tensor, start_timestep) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the diffusion model.\n",
    "\n",
    "        Args:\n",
    "            x_0 (torch.Tensor): The original image (image or feature vector).\n",
    "            start_timestep (int): The starting timestep for sampling.\n",
    "        Returns:\n",
    "            torch.Tensor: The generated sample(s).\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "\n",
    "        x_t = self.q_sample(\n",
    "            x_0=x_0,\n",
    "            t=torch.tensor([start_timestep]).to(x_0.device),\n",
    "            noise=self.noise_fn(x_0),\n",
    "        )\n",
    "\n",
    "        for i in tqdm(\n",
    "            reversed(range(0, start_timestep)),\n",
    "            desc=\"sampling loop\",\n",
    "            total=start_timestep,\n",
    "        ):\n",
    "            x_t = self.p_sample(x_t, i)\n",
    "\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0P3otVhjsbQ8"
   },
   "source": [
    "### Training\n",
    "\n",
    "We are now going to train, just launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJCrRk2Msawl"
   },
   "outputs": [],
   "source": [
    "normal_dataset = BraTSDataset(\n",
    "    root_dir,\n",
    "    mode=\"train\",\n",
    "    transform=transforms.Lambda(lambda x: 2 * (x - x.min()) / (x.max() - x.min()) - 1),\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(normal_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "unet = Unet(dim=64, channels=1, dim_mults=(1, 1, 2))\n",
    "\n",
    "model = AnoDDPMDiffusionModel(\n",
    "    Unet=unet,\n",
    "    timesteps=timesteps,\n",
    "    beta_fn=quadratic_beta_schedule,\n",
    "    noise_type=\"simplex\",\n",
    ")\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, epochs=10, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Trains the diffusion model.\n",
    "\n",
    "    Args:\n",
    "        model (DiffusionModel): The diffusion model to train.\n",
    "        train_dataloader (DataLoader): DataLoader for training data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        device (str): Device to use for training (\"cuda\" or \"cpu\").\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(\n",
    "            tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        ):\n",
    "            x_0 = batch[\"img\"].to(device).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.noise_loss(x_0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch + 1}/{epochs}], Step [{batch_idx + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\"\n",
    "                )\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pt\")\n",
    "\n",
    "\n",
    "train(model, train_dataloader, epochs=40, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKZeemYaj_fm"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv8AXtZQkD9y"
   },
   "source": [
    "We are now going to predict, just launch the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SatSBzeFkDa5"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "x_0 = abnormal_sample[\"img\"].unsqueeze(0).to(\"mps:0\").float()\n",
    "seg = abnormal_sample[\"mask\"].unsqueeze(0).to(\"mps:0\").float()\n",
    "\n",
    "start_timestep = 240\n",
    "generated_sample = model.predict(x_0, start_timestep)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(10, 5))\n",
    "seg = abnormal_sample[\"mask\"].unsqueeze(0).to(\"mps:0\").float()\n",
    "\n",
    "axes[0].imshow(x_0.squeeze().cpu().numpy(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(\"Original Image\")\n",
    "x\n",
    "axes[1].imshow(generated_sample.squeeze().cpu().numpy(), cmap=\"gray\", vmin=-1, vmax=1)\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(\"Generated Sample\")\n",
    "\n",
    "axes[2].imshow(seg.squeeze().cpu().numpy(), cmap=\"seismic\", vmin=-1, vmax=1)\n",
    "axes[2].set_title(\"Segmentation Mask\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "axes[3].imshow(\n",
    "    (x_0.squeeze() - generated_sample.squeeze()).pow(2).cpu().numpy(), cmap=\"seismic\"\n",
    ")\n",
    "axes[3].set_title(\"Difference\")\n",
    "axes[3].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGstqgImkPjO"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBh5cgi9tspN"
   },
   "source": [
    "**Dice**\n",
    "\n",
    "The Dice score (also known as the Dice Similarity Coefficient, DSC) is a statistical metric used to measure the overlap between two sets. \n",
    "It is commonly used in image segmentation tasks to quantify how well the predicted segmentation matches the ground truth.\n",
    "\n",
    "Given two binary sets $A$ and $B$ (e.g., predicted and ground truth masks), the Dice score is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{Dice}(A,B) = \\frac{2 | A \\cap B |}{ |A| + |B|}\n",
    "$$\n",
    "\n",
    "In terms of true positives (TP), false positives (FP), and false negatives (FN), for binary classification, it can be written as:\n",
    "\n",
    "$$\n",
    "\\mathrm{Dice} = \\frac{2 TP}{ 2 TP + FP + FN}\n",
    "$$\n",
    "\n",
    "The score ranges from 0 (no overlap) to 1 (perfect overlap).\n",
    "\n",
    "For a deeper explanation follow: https://metrics-reloaded.dkfz.de/metric?id=dsc\n",
    "\n",
    "**Best Dice ($\\lceil \\text{Dice} \\rceil$)**\n",
    "\n",
    "Since our anomaly maps are unthresholded, we can't directly computing the Dice score from them, as it requires binary maps. \n",
    "One possibility would be to binarise the maps, using a threshold that we could define. \n",
    "Another possibility is to use metrics that do not strongly rely on thresholds. \n",
    "\n",
    "The Best Dice, or $\\lceil \\text{Dice} \\rceil$, is an estimate of the best possible Dice for any given threshold. \n",
    "In practice, we iterate other thresholds, binarise the anomaly map for each threshold and compute the corresponding Dice score. \n",
    "The $\\lceil \\text{Dice} \\rceil$ corresponds to the maximum Dice obtained across all thresholds.\n",
    "\n",
    "\n",
    "**Pixelwise Average Precision (AP)**\n",
    "\n",
    "The Average Precision (AP) is a widely used metric in object detection, segmentation, and retrieval tasks. \n",
    "It summarizes the precision-recall curve as the area under the curve, giving a single number that captures the trade-off between precision and recall across different thresholds.\n",
    "\n",
    "Formally, \n",
    "$$\\text{precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}} = \\mathbb{P}[\\hat{y} = 1 | y = 1], \\quad \\text{recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}} = \\mathbb{P}[y = 1 | \\hat{y} = 1],$$\n",
    "\n",
    "and if we denote by $P_n, R_n$ the precision and recall at the $n$-th threshold: \n",
    "\n",
    "$$AP = \\sum_n (R_n - R_{n-1}) P_n.$$\n",
    "\n",
    "For more explanation, you can go check the [scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/iolag/UPD_study/blob/main/UPD_study/utilities/metrics.py\n",
    "\n",
    "\n",
    "def compute_dice(preds: np.ndarray, targets: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Sorensen-Dice coefficient:\n",
    "\n",
    "    dice = 2 * TP / (2 * TP + FP + FN)\n",
    "\n",
    "    :param preds: An array of predicted anomaly scores.\n",
    "    :param targets: An array of ground truth labels.\n",
    "    \"\"\"\n",
    "    preds, targets = np.array(preds), np.array(targets)\n",
    "\n",
    "    # Check if predictions and targets are binary\n",
    "    if not np.all(np.logical_or(preds == 0, preds == 1)):\n",
    "        raise ValueError(\"Predictions must be binary\")\n",
    "    if not np.all(np.logical_or(targets == 0, targets == 1)):\n",
    "        raise ValueError(\"Targets must be binary\")\n",
    "\n",
    "    # Compute Dice\n",
    "    dice = 2 * np.sum(preds[targets == 1]) / (np.sum(preds) + np.sum(targets))\n",
    "\n",
    "    return dice\n",
    "\n",
    "\n",
    "def compute_best_dice(\n",
    "    preds: np.ndarray,\n",
    "    targets: np.ndarray,\n",
    "    n_thresh: float = 100,\n",
    "    num_processes: int = 4,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Compute the best dice score for n_thresh thresholds.\n",
    "\n",
    "    :param predictions: An array of predicted anomaly scores.\n",
    "    :param targets: An array of ground truth labels.\n",
    "    :param n_thresh: Number of thresholds to check.\n",
    "    \"\"\"\n",
    "    preds, targets = np.array(preds), np.array(targets)\n",
    "\n",
    "    thresholds = np.linspace(preds.max(), preds.min(), n_thresh)\n",
    "\n",
    "    with Pool(num_processes) as pool:\n",
    "        fn = partial(_dice_multiprocessing, preds, targets)\n",
    "        scores = pool.map(fn, thresholds)\n",
    "\n",
    "    scores = np.stack(scores, 0)\n",
    "    max_dice = scores.max()\n",
    "    max_thresh = thresholds[scores.argmax()]\n",
    "    return max_dice, max_thresh\n",
    "\n",
    "\n",
    "def _dice_multiprocessing(\n",
    "    preds: np.ndarray, targets: np.ndarray, threshold: float\n",
    ") -> float:\n",
    "    return compute_dice(np.where(preds > threshold, 1, 0), targets)\n",
    "\n",
    "\n",
    "def compute_average_precision(predictions, targets):\n",
    "    \"\"\"\n",
    "    Compute Average Precision\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Anomaly scores\n",
    "        targets (torch.Tensor): Segmentation map or target label, must be binary\n",
    "    \"\"\"\n",
    "    if (targets - targets.int()).sum() > 0.0:\n",
    "        raise RuntimeError(\"targets for AP must be binary\")\n",
    "    ap = average_precision_score(targets.reshape(-1), predictions.reshape(-1))\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NiDVglvkDAm"
   },
   "outputs": [],
   "source": [
    "pred_mask = (generated_sample > 0.3).float()\n",
    "target_mask = (seg > 0.1).float()\n",
    "dice = compute_best_dice(pred_mask, target_mask)\n",
    "print(f\"Dice Score: {dice.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDwMY76gtr8a"
   },
   "outputs": [],
   "source": [
    "ap = compute_average_precision(pred_mask, target_mask)\n",
    "print(f\"Average Precision (AP) at pixel level: {ap.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate on the test set how is performing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(abnormal_dataset, batch_size=15, shuffle=False)\n",
    "\n",
    "\n",
    "def evaluation(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    start_timestep=200,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"AnoDDPM evaluation function.\n",
    "\n",
    "    Args:\n",
    "        model (AnoDDPMDiffusionModel): The trained AnoDDPM model.\n",
    "        test_dataloader (DataLoader): DataLoader for test data.\n",
    "        device (str): Device to use for evaluation (\"cuda\" or \"cpu\").\n",
    "    Returns:\n",
    "        dict: A dictionary containing the average Dice score and Average Precision (AP) score.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    dice_scores = []\n",
    "    ap_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            x_0 = batch[\"img\"].to(device).float()\n",
    "            seg = batch[\"mask\"].to(device).float()\n",
    "\n",
    "            # Generate sample\n",
    "            generated_sample = model.predict(x_0, start_timestep=start_timestep)\n",
    "\n",
    "            for i in range(len(generated_sample)):\n",
    "                sample = generated_sample[i]\n",
    "\n",
    "                # Compute residual\n",
    "                residual = x_0[i].squeeze() - sample.squeeze()\n",
    "                seg_sample = seg[i].squeeze()\n",
    "\n",
    "                # Compute Dice score\n",
    "                pred_mask = (residual.pow(2) > 0.1).float()\n",
    "                target_mask = (seg_sample > 0.1).float()\n",
    "                dice = compute_dice(pred_mask, target_mask)\n",
    "                dice_scores.append(dice)\n",
    "\n",
    "                # Compute Average Precision (AP)\n",
    "                ap = compute_average_precision(residual.pow(2), target_mask)\n",
    "                ap_scores.append(ap)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\"Dice Score\": dice_scores, \"Average Precision (AP)\": ap_scores}\n",
    "    )\n",
    "\n",
    "\n",
    "results = evaluation(model, test_loader, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WV0Vb2X5Zp0P"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_json": true,
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
